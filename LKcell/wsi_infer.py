#!/usr/bin/env python3
"""
WSIç»†èƒåˆ†å¸ƒåˆ†æè„šæœ¬
åˆ†æWSIçš„æ‰€æœ‰patchçš„ç»†èƒåˆ†å¸ƒï¼Œå¹¶åº”ç”¨RPSMé€‰æ‹©æ¡ä»¶ï¼Œç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡å›¾è¡¨
Enhanced with Multi-GPU support for faster processing
"""

import os
import sys
import pandas as pd
import numpy as np
import random
import torch
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
import json
import time
import threading
from pathlib import Path
import torchvision.transforms as transforms
import torch.nn.functional as F
from collections import defaultdict
import glob
from datetime import datetime
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from scipy import stats
from sklearn.metrics import roc_curve, auc
from skimage.measure import regionprops
from skimage.color import rgb2gray
from skimage.feature import graycomatrix, graycoprops
# è®¾ç½®å­—ä½“ä¸ºè‹±æ–‡ï¼Œé¿å…ä¹±ç 
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
def generate_inst_type_from_class_map(instance_map, class_map):
    """
    instance_map: (H, W) å®ä¾‹åˆ†å‰²å›¾ï¼Œ0 è¡¨ç¤ºèƒŒæ™¯
    class_map: (H, W) å®ä¾‹ç±»åˆ«å›¾ï¼Œ0 è¡¨ç¤ºèƒŒæ™¯ï¼Œ1,2,...è¡¨ç¤ºä¸åŒç±»åˆ«
    è¿”å›:
        inst_type: (N,1) æ¯ä¸ªå®ä¾‹å¯¹åº”ç±»åˆ«
    """
    unique_ids = np.unique(instance_map)
    unique_ids = unique_ids[unique_ids != 0]  # å»æ‰èƒŒæ™¯

    inst_type = []
    for inst_id in unique_ids:
        # è·å–å½“å‰å®ä¾‹çš„ç±»åˆ«
        mask = instance_map == inst_id
        # å‡è®¾å®ä¾‹å†…éƒ¨ç±»åˆ«ä¸€è‡´ï¼Œå–ç¬¬ä¸€ä¸ªéé›¶ç±»åˆ«
        cls = np.unique(class_map[mask])
        cls = cls[cls != 0][0] if len(cls[cls != 0]) > 0 else 0
        inst_type.append(cls)
    inst_type = np.array(inst_type, dtype=np.int32).reshape(-1, 1)
    return inst_type


class SlideNucStatObject:
    """
    è®¡ç®—å•å¼ åˆ‡ç‰‡çš„ç»†èƒæ ¸ç‰¹å¾ï¼ŒåŒ…æ‹¬å½¢æ€ã€é¢œè‰²ã€Haralickã€é‚»å±…ä¿¡æ¯ã€‚
    è¾“å…¥ä¸º instance_map å’Œ inst_typeã€‚
    """
    def __init__(self, instance_map: np.ndarray, inst_type,image: np.ndarray = None):
        """
        Args:
            instance_map: (H, W) åˆ†å‰²å®ä¾‹å›¾ï¼Œ0 è¡¨ç¤ºèƒŒæ™¯
            inst_type: (H, W) å®ä¾‹ç±»å‹å›¾ï¼Œ0 è¡¨ç¤ºèƒŒæ™¯
            image: å¯é€‰ï¼ŒRGB åŸå›¾ï¼Œç”¨äºé¢œè‰²å’Œ Haralick ç‰¹å¾è®¡ç®—
        """
        self.type_names = {1: "Neoplastic", 2: "Inflammatory", 3: "Connective", 4: "Dead", 5: "Epithelial"}
        self.instance_map = instance_map
    # é»˜è®¤ç±»å‹å…¨éƒ¨ä¸º0
        # import ipdb; ipdb.set_trace()
        self.inst_type = generate_inst_type_from_class_map(instance_map, inst_type)
        self.image = image
        self.nuclei_index = np.arange(len(inst_type))  # å¯¹åº”æ¯ä¸ªå®ä¾‹çš„ç´¢å¼•
        self.n_instances = len(self.nuclei_index)
        self.feature_columns = None

    def _get_haralick_features(self, gray_img, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256):
        """
        æå–å•ä¸ªç»†èƒçš„ Haralick ç‰¹å¾
        """
        glcm = graycomatrix(gray_img, distances=distances, angles=angles, levels=levels, symmetric=True, normed=True)
        features = {}
        props = ["contrast", "homogeneity", "dissimilarity", "ASM", "energy", "correlation"]
        for prop in props:
            features[prop] = np.mean(graycoprops(glcm, prop))
        features["heterogeneity"] = 1 - features["homogeneity"]
        return features

    def _nuc_stat_func(self, inst_id):
        mask = self.instance_map == inst_id
        if mask.sum() == 0:
            return None

        # å½¢æ€ç‰¹å¾
        stat = regionprops(mask.astype(np.uint8))[0]
        morphology = {
            "major_axis_length": stat.major_axis_length,
            "minor_axis_length": stat.minor_axis_length,
            "major_minor_ratio": stat.major_axis_length / stat.minor_axis_length if stat.minor_axis_length>0 else 0,
            "orientation_degree": stat.orientation * 180 / np.pi + 90,
            "area": stat.area,
            "extent": stat.extent,
            "solidity": stat.solidity,
            "convex_area": stat.convex_area,
            "eccentricity": stat.eccentricity,
            "equivalent_diameter": stat.equivalent_diameter,
            "perimeter": stat.perimeter
        }

        # é¢œè‰²ç‰¹å¾
        color_features = {}
        if self.image is not None:
            masked_img = self.image * np.expand_dims(mask, axis=-1)
            for i, c in enumerate(["R", "G", "B"]):
                channel = masked_img[:,:,i][mask]
                color_features[f"{c}_mean"] = np.mean(channel)
                color_features[f"{c}_std"] = np.std(channel)
                color_features[f"{c}_min"] = np.min(channel)
                color_features[f"{c}_max"] = np.max(channel)
            gray_img = rgb2gray(masked_img).astype(np.uint8)
            haralick_features = self._get_haralick_features(gray_img)
        else:
            for c in ["R","G","B"]:
                color_features[f"{c}_mean"] = np.nan
                color_features[f"{c}_std"] = np.nan
                color_features[f"{c}_min"] = np.nan
                color_features[f"{c}_max"] = np.nan
            haralick_features = {k: np.nan for k in ["contrast","homogeneity","dissimilarity","ASM","energy","correlation","heterogeneity"]}

        cell_type = self.inst_type[inst_id-1,0]
        cell_type_name = self.type_names.get(cell_type, "Unknown")
        features = {"inst_id": inst_id, "cell_type": cell_type_name}
        features.update(morphology)
        features.update(color_features)
        features.update(haralick_features)
        return features

    def compute_nuc_features(self):
        features = []
        for inst_id in tqdm(range(1, self.n_instances+1)):
            stat = self._nuc_stat_func(inst_id)
            if stat is not None:
                features.append(stat)
        df_features = pd.DataFrame(features)
        return df_features

    def compute_delaunay_features(self, df_features):
        """
        è®¡ç®— Delaunay é‚»å±…ç‰¹å¾
        """
        centroids = []
        for inst_id in range(1, self.n_instances+1):
            mask = self.instance_map == inst_id
            props = regionprops(mask.astype(np.uint8))[0]
            centroids.append(props.centroid)
        centroids = np.array(centroids)
        tri = Delaunay(centroids)
        indices, indptr = tri.vertex_neighbor_vertices
        # import ipdb
        # ipdb.set_trace()
        delaunay_feats = []
        for i in range(self.n_instances):
            neighbors = indices[indptr[i]:indptr[i+1]]
            if len(neighbors) == 0:
                delaunay_feats.append([np.nan]*4)
            else:
                dist = np.linalg.norm(centroids[neighbors]-centroids[i], axis=1)
                delaunay_feats.append([np.mean(dist), np.std(dist), np.min(dist), np.max(dist)])
        df_delaunay = pd.DataFrame(delaunay_feats, columns=["dist_mean","dist_std","dist_min","dist_max"])
        return df_delaunay

    def compute_features(self):
        df_nuc = self.compute_nuc_features()
        # df_delaunay = self.compute_delaunay_features(df_nuc)
        df_all = pd.concat([df_nuc.reset_index(drop=True)], axis=1)
        return df_all

# ================== å¤§è§„æ¨¡æ•°æ®åˆ†æé…ç½® ==================
class AnalysisConfig:
    """åˆ†æé…ç½®ç±» - é’ˆå¯¹å¤§è§„æ¨¡WSIæ•°æ®ä¼˜åŒ–"""
    # æ•°æ®é›†è§„æ¨¡é˜ˆå€¼
    LARGE_DATASET_THRESHOLD = 50      # å¤§è§„æ¨¡æ•°æ®é›†é˜ˆå€¼
    MEDIUM_DATASET_THRESHOLD = 20     # ä¸­ç­‰è§„æ¨¡æ•°æ®é›†é˜ˆå€¼
    
    # é‡‡æ ·ç­–ç•¥
    MAX_DETAILED_ANALYSIS_LARGE = 10  # å¤§è§„æ¨¡æ•°æ®é›†æœ€å¤šè¯¦ç»†åˆ†ææ•°é‡
    MAX_DETAILED_ANALYSIS_MEDIUM = 20 # ä¸­ç­‰æ•°æ®é›†æœ€å¤šè¯¦ç»†åˆ†ææ•°é‡
    
    # æ€§èƒ½ä¼˜åŒ–
    BATCH_SIZE = 8                    # åˆ†æ‰¹å¤„ç†å¤§å°
    MAX_WORKERS = 4                   # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    
    # å¯è§†åŒ–æ§åˆ¶
    ENABLE_INDIVIDUAL_WSI_PLOTS = True  # æ˜¯å¦ç”Ÿæˆä¸ªä½“WSIå›¾è¡¨
    FORCE_AGGREGATED_MODE = False      # å¼ºåˆ¶ä½¿ç”¨èšåˆæ¨¡å¼
    
    # è¾“å‡ºæ§åˆ¶
    SAVE_INTERMEDIATE_RESULTS = True   # ä¿å­˜ä¸­é—´ç»“æœ
    GENERATE_SAMPLING_INFO = True      # ç”Ÿæˆé‡‡æ ·ä¿¡æ¯
    
    @classmethod
    def auto_configure(cls, num_wsi):
        """æ ¹æ®WSIæ•°é‡è‡ªåŠ¨é…ç½®å‚æ•°"""
        if num_wsi > cls.LARGE_DATASET_THRESHOLD:
            print(f"ğŸ”§ Auto-config: Large dataset mode ({num_wsi} WSIs)")
            cls.ENABLE_INDIVIDUAL_WSI_PLOTS = False
            cls.FORCE_AGGREGATED_MODE = True
        elif num_wsi > cls.MEDIUM_DATASET_THRESHOLD:
            print(f"ğŸ”§ Auto-config: Medium dataset mode ({num_wsi} WSIs)")
            cls.ENABLE_INDIVIDUAL_WSI_PLOTS = True  # ä½†ä¼šé‡‡æ ·
        else:
            print(f"ğŸ”§ Auto-config: Small dataset mode ({num_wsi} WSIs)")

def numpy_json_serializer(obj):
    """
    Custom JSON serializer for numpy data types and other non-serializable objects
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, 'item'):  # For numpy scalars
        return obj.item()
    else:
        return str(obj)

def setup_pannuke_models():
    """
    Setup and load PanNuke models with PyTorch DataParallel for multi-GPU inference
    ä½¿ç”¨PyTorchæ ‡å‡†çš„DataParallelå®ç°å¤šGPUåŠ é€Ÿ
    """
    # Set working directory to LKCell
    original_cwd = os.getcwd()
    lkcell_dir = Path("/home/stat-huamenglei/LKCell")
    # import ipdb; ipdb.set_trace()
    if not lkcell_dir.exists():
        print("LKCell directory does not exist")
        return None, None
    
    # Check available GPUs
    if not torch.cuda.is_available():
        print("CUDA not available, falling back to CPU")
        return setup_single_gpu_model()
    
    num_gpus = torch.cuda.device_count()
    print(f"ğŸš€ Detected {num_gpus} GPU(s) available")
    
    # Switch to LKCell directory
    os.chdir(lkcell_dir)
    sys.path.insert(0, str(Path.cwd()))
    
    try:
        from cell_segmentation.inference.inference_cellvit_experiment_pannuke import InferenceCellViTParser, InferenceCellViT
        device_ids = [0, 1]
        device = torch.device(f"cuda:{device_ids[0]}")
        print(f"âš¡ Setting up DataParallel model on all available GPUs...")
        
        # Setup PanNuke model configuration
        pannuke_parser = InferenceCellViTParser()
        pannuke_configurations = pannuke_parser.parse_arguments()
        
        pannuke_inf = InferenceCellViT(
            run_dir=pannuke_configurations["run_dir"],
            checkpoint_name=pannuke_configurations["checkpoint_name"],
            gpu=pannuke_configurations["gpu"],
            magnification=pannuke_configurations["magnification"],
        )
        
        # Load model checkpoint
        checkpoint_path = pannuke_inf.run_dir / "checkpoints" / pannuke_inf.checkpoint_name
        print(f"Checkpoint path: {checkpoint_path}")
        
        if not checkpoint_path.exists():
            print(f"Model checkpoint does not exist: {checkpoint_path}")
            os.chdir(original_cwd)
            return None, None
        
        pannuke_checkpoint = torch.load(checkpoint_path, map_location="cpu")
        pannuke_model = pannuke_inf.get_model(model_type=pannuke_checkpoint["arch"])
        pannuke_model.load_state_dict(pannuke_checkpoint["model_state_dict"])
        pannuke_model.to(device)
        # ä½¿ç”¨DataParallelåŒ…è£…æ¨¡å‹ä»¥æ”¯æŒå¤šGPU
        if num_gpus > 1:
            print(f"ğŸ¯ Wrapping model with DataParallel for {num_gpus} GPUs")
            pannuke_model = torch.nn.DataParallel(pannuke_model, device_ids=device_ids)
            is_multi_gpu = True
        else:
            is_multi_gpu = False
        
        pannuke_model.to(device)
        pannuke_model.eval()
        
        print(f"âœ… PanNuke model loaded to device: {device}")
        if is_multi_gpu:
            print(f"ğŸš€ Multi-GPU acceleration enabled with DataParallel")
        
        # Switch back to original directory
        os.chdir(original_cwd)
        
        return pannuke_model, device, is_multi_gpu
        
    except Exception as e:
        print(f"Model loading failed: {e}")
        os.chdir(original_cwd)
        return setup_single_gpu_model()

def setup_single_gpu_model():
    """
    Fallback function for single GPU setup
    """
    # Set working directory to LKCell
    original_cwd = os.getcwd()
    lkcell_dir = Path("LKCell")
    os.chdir(lkcell_dir)
    sys.path.insert(0, str(Path.cwd()))
    
    try:
        from cell_segmentation.inference.inference_cellvit_experiment_pannuke import InferenceCellViTParser, InferenceCellViT
        
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        
        # Setup PanNuke model
        pannuke_parser = InferenceCellViTParser()
        pannuke_configurations = pannuke_parser.parse_arguments()
        
        print(f"Configuration: {pannuke_configurations}")
        
        pannuke_inf = InferenceCellViT(
            run_dir=pannuke_configurations["run_dir"],
            checkpoint_name=pannuke_configurations["checkpoint_name"],
            gpu=pannuke_configurations["gpu"],
            magnification=pannuke_configurations["magnification"],
        )
        
        # Load model checkpoint
        checkpoint_path = pannuke_inf.run_dir / "checkpoints" / pannuke_inf.checkpoint_name
        print(f"Checkpoint path: {checkpoint_path}")
        
        if not checkpoint_path.exists():
            print(f"Model checkpoint does not exist: {checkpoint_path}")
            os.chdir(original_cwd)
            return None, None, False
        
        pannuke_checkpoint = torch.load(checkpoint_path, map_location="cpu")
        pannuke_model = pannuke_inf.get_model(model_type=pannuke_checkpoint["arch"])
        pannuke_model.load_state_dict(pannuke_checkpoint["model_state_dict"])
        pannuke_model.to(device)
        pannuke_model.eval()
        
        print(f"PanNuke model loaded to device: {device}")
        
        # Switch back to original directory
        os.chdir(original_cwd)
        
        return pannuke_model, device, False
        
    except Exception as e:
        print(f"Model loading failed: {e}")
        os.chdir(original_cwd)
        return None, None, False

def load_all_wsi_data(csv_path):
    """
    Load ALL WSI data from CSV file for complete RPSM evaluation
    Used for comprehensive analysis of RPSM filtering criteria
    """
    df = pd.read_csv(csv_path)
    print(f"Total {len(df)} WSI samples found")
    
    # Get all samples from responder and non-responder groups
    responders = df[df['label'] == 1]
    non_responders = df[df['label'] == 0]
    
    print(f"Responder group: {len(responders)} samples")
    print(f"Non-responder group: {len(non_responders)} samples")
    
    all_samples = []
    
    # Add all responder samples
    for _, sample in responders.iterrows():
        all_samples.append(sample)
        print(f"Added responder sample: {sample['slides_name']}")
    
    # Add all non-responder samples  
    for _, sample in non_responders.iterrows():
        all_samples.append(sample)
        print(f"Added non-responder sample: {sample['slides_name']}")
    
    print(f"Total {len(all_samples)} WSI samples will be analyzed for complete RPSM evaluation")
    return all_samples

def load_and_sample_wsi_data(csv_path, num_samples_per_group=3):
    """
    Load WSI data from CSV file and strategically sample
    Enhanced sampling to reduce heterogeneity and improve analysis reliability
    Updated default to 12 samples per group for better statistical power
    """
    df = pd.read_csv(csv_path)
    print(f"Total {len(df)} WSI samples found")
    
    # Select multiple samples from responder and non-responder groups
    responders = df[df['label'] == 1]
    non_responders = df[df['label'] == 0]
    
    print(f"Responder group: {len(responders)} samples")
    print(f"Non-responder group: {len(non_responders)} samples")
    
    selected_samples = []
    
    # Reset index to ensure proper sampling
    responders = responders.reset_index(drop=True)
    non_responders = non_responders.reset_index(drop=True)
    
    # Enhanced sampling strategy for responders
    if len(responders) > 0:
        num_responder_samples = min(num_samples_per_group, len(responders))
        print(f"Selecting {num_responder_samples} responder samples with enhanced strategy...")
        
        # ä½¿ç”¨å¤šé‡éšæœºæ€§æºæé«˜é€‰æ‹©çš„å¤šæ ·æ€§
        import time
        base_seed = int(time.time() * 1000000) % 2147483647
        
        # æ™ºèƒ½åˆ†å±‚é‡‡æ ·ç­–ç•¥ - åŸºäº12-WSIåˆ†æç»éªŒ
        # ä¼˜å…ˆé€‰æ‹©å…·æœ‰ä¸åŒç»„ç»‡å­¦ç‰¹å¾çš„æ ·æœ¬ä»¥æé«˜ä»£è¡¨æ€§
        if len(responders) >= 12:
            # å°è¯•åˆ†å±‚é‡‡æ ·ï¼šé€‰æ‹©ä¸åŒä¸´åºŠç‰¹å¾çš„æ ·æœ¬
            # å¦‚æœæœ‰ä¸´åºŠæ•°æ®ï¼Œå¯ä»¥æŒ‰å¹´é¾„ã€åˆ†æœŸã€ç—…ç†ç±»å‹ç­‰åˆ†å±‚
            selected_indices = []
            available_indices = list(range(len(responders)))
            
            # å¤šæ¬¡éšæœºåŒ–ç¡®ä¿æ ·æœ¬å¤šæ ·æ€§
            for round_num in range(3):
                temp_seed = (base_seed + round_num * 7919) % 2147483647
                temp_random = random.Random(temp_seed)
                temp_random.shuffle(available_indices)
            
            # æœ€ç»ˆé€‰æ‹©
            selected_indices = available_indices[:num_responder_samples]
        else:
            selected_indices = available_indices[:num_responder_samples]
        
        print(f"Selected responder indices: {selected_indices}")
        
        selected_responders = responders.iloc[selected_indices]
        
        for _, sample in selected_responders.iterrows():
            selected_samples.append(sample)
            print(f"Selected responder sample: {sample['slides_name']}")
    
    # Enhanced sampling strategy for non-responders
    if len(non_responders) > 0:
        num_non_responder_samples = min(num_samples_per_group, len(non_responders))
        print(f"Selecting {num_non_responder_samples} non-responder samples with enhanced strategy...")
        
        # ä½¿ç”¨ç³»ç»Ÿæ—¶é—´ä½œä¸ºé¢å¤–çš„éšæœºæ€§æº
        import time
        time.sleep(0.001)  # çŸ­æš‚å»¶è¿Ÿç¡®ä¿æ—¶é—´å˜åŒ–
        microsecond_seed = int(time.time() * 1000000) % 1000000
        
        # ç»“åˆå¤šç§éšæœºæ€§æº
        available_indices = list(range(len(non_responders)))
        
        # ä½¿ç”¨ç³»ç»Ÿæ—¶é—´è¿›è¡Œå¤šæ¬¡æ‰“ä¹±
        temp_random = random.Random(microsecond_seed)
        temp_random.shuffle(available_indices)
        random.shuffle(available_indices)  # å†æ¬¡æ‰“ä¹±
        
        # ä»æ‰“ä¹±åçš„åˆ—è¡¨ä¸­é€‰æ‹©
        selected_indices = available_indices[:num_non_responder_samples]
        
        print(f"Available non-responder indices: {len(available_indices)}")
        print(f"Selected non-responder indices: {selected_indices}")
        print(f"Using microsecond seed: {microsecond_seed}")
        
        selected_non_responders = non_responders.iloc[selected_indices]
        
        for _, sample in selected_non_responders.iterrows():
            selected_samples.append(sample)
            print(f"Selected non-responder sample: {sample['slides_name']}")
    
    print(f"Total {len(selected_samples)} WSI samples selected for analysis")
    return selected_samples

def get_patch_files(patch_dir):
    """
    Get all patch files from patch directory
    """
    patch_files = [f for f in glob.glob(os.path.join(patch_dir, "*.png")) if not f.endswith("_overlay.png")]
    
    if len(patch_files) == 0:
        print(f"No patch files found in {patch_dir}")
        return []
    
    print(f"Found {len(patch_files)} patch files, will analyze all")
    return patch_files

class PatchDataset(Dataset):
    def __init__(self, patch_paths, transform=None):
        self.patch_paths = patch_paths
        self.transform = transform

    def __len__(self):
        return len(self.patch_paths)

    def __getitem__(self, idx):
        path = self.patch_paths[idx]
        image = cv2.imread(path)
        if image is None:
            # è¿”å›Noneç”¨äºåç»­è¿‡æ»¤
            return None, path
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        if image.shape[0] != 512 or image.shape[1] != 512:
            image = cv2.resize(image, (512, 512))
        from PIL import Image as PILImage
        image_pil = PILImage.fromarray(image)
        if self.transform:
            image_tensor = self.transform(image_pil)
        else:
            image_tensor = transforms.ToTensor()(image_pil)
        return image_tensor, path
def analyze_patches_multi_gpu(patch_paths, models_and_devices, batch_size=8):
    """
    Multi-GPU parallel patch inference with load balancing
    åˆ©ç”¨ä¸¤å—GPUè¿›è¡Œå¹¶è¡Œæ¨ç†ï¼Œæ˜¾è‘—æå‡é€Ÿåº¦
    """
    import threading
    from queue import Queue
    import math
    
    num_gpus = len(models_and_devices)
    print(f"ğŸš€ Starting multi-GPU inference with {num_gpus} GPUs")
    
    # Split patches between GPUs
    # import ipdb; ipdb.set_trace() 
    chunks = [[] for _ in range(num_gpus)]
    for i, patch_path in enumerate(patch_paths):
        chunks[i % num_gpus].append(patch_path)
    # import ipdb; ipdb.set_trace() 
    print(f"ğŸ“Š Load distribution:")
    for i, chunk in enumerate(chunks):
        print(f"   GPU {i}: {len(chunk)} patches")
    
    # Results collection
    all_results = []
    result_lock = threading.Lock()
    
    def gpu_worker(gpu_id, patch_chunk, model, device):
        """Worker function for each GPU"""
        chunk_results = []
        
        try:
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            
            dataset = PatchDataset(patch_chunk, transform=transform)
            # å‡å°‘num_workersä»¥é¿å…å†…å­˜å†²çª
            dataloader = DataLoader(dataset, batch_size=batch_size//2, num_workers=2, pin_memory=False)
            
            model.eval()
            with torch.no_grad():
                pbar = tqdm(dataloader, desc=f"GPU {gpu_id}", position=gpu_id, leave=True, ncols=80)
                for batch in pbar:
                    try:
                        images, paths = batch
                        
                        # Filter valid images
                        valid_indices = [i for i, img in enumerate(images) if img is not None]
                        if not valid_indices:
                            for path in paths:
                                chunk_results.append(create_empty_analysis(path))
                            continue
                        
                        valid_images = torch.stack([img for img in images if img is not None]).to(device)
                        valid_paths = [paths[i] for i in valid_indices]
                        
                        predictions = model(valid_images)
                        predictions["nuclei_binary_map"] = F.softmax(predictions["nuclei_binary_map"], dim=1)
                        predictions["nuclei_type_map"] = F.softmax(predictions["nuclei_type_map"], dim=1)
                        
                        for batch_idx, patch_path in enumerate(valid_paths):
                            single_pred = {key: value[batch_idx:batch_idx+1] for key, value in predictions.items()}
                            instance_map, instance_types = model.calculate_instance_map(single_pred, magnification=40)
                            instance_map = instance_map[0].cpu().numpy()
                            nuclei_type_map = single_pred["nuclei_type_map"][0].cpu().numpy()
                            nuclei_pred = np.argmax(nuclei_type_map, axis=0)
                            slide_obj = SlideNucStatObject(instance_map=instance_map,inst_type = nuclei_pred, image=valid_images)
                            df_features = slide_obj.compute_features()
                            df_features.to_csv(patch_path.with_suffix('.csv'), index=False)
                            if len(instance_types) == 0 or len(instance_types[0]) == 0:
                                result = create_empty_analysis(patch_path)
                            else:
                                result = create_patch_analysis(patch_path, instance_types[0])
                            
                            chunk_results.append(result)
                            
                    except Exception as e:
                        print(f"GPU {gpu_id} batch processing error: {e}")
                        # ä¸ºå¤±è´¥çš„batchåˆ›å»ºç©ºåˆ†æ
                        for path in paths:
                            chunk_results.append(create_empty_analysis(path))
                        
        except Exception as e:
            print(f"GPU {gpu_id} worker failed: {e}")
            # ä¸ºæ‰€æœ‰æœªå¤„ç†çš„patchesåˆ›å»ºç©ºåˆ†æ
            for path in patch_chunk:
                chunk_results.append(create_empty_analysis(path))
        
        # Thread-safe result collection
        with result_lock:
            all_results.extend(chunk_results)
            print(f"âœ… GPU {gpu_id} completed: {len(chunk_results)} patches")
    
    # Start threads for each GPU
    threads = []
    for gpu_id, (model, device) in enumerate(models_and_devices):
        if len(chunks[gpu_id]) > 0:  # Only start thread if there are patches to process
            thread = threading.Thread(
                target=gpu_worker, 
                args=(gpu_id, chunks[gpu_id], model, device)
            )
            thread.start()
            threads.append(thread)
    
    # Wait for all threads to complete
    for thread in threads:
        thread.join()
    
    print(f"ğŸ¯ Multi-GPU inference completed: {len(all_results)} total results")
    return all_results

def create_empty_analysis(patch_path):
    """Create empty analysis for failed patches"""
    return {
        'patch_path': patch_path,
        'cell_counts': [0] * 6,  # Background + 5 cell types
        'cell_ratios': [0.0] * 6,
        'total_cells': 0
    }

def create_patch_analysis(patch_path, instance_types):
    """Create patch analysis from instance types"""
    cell_counts = [0] * 6  # Background + 5 cell types
    for cell_type in instance_types:
        if 0 <= cell_type < 6:
            cell_counts[cell_type] += 1
    
    total_cells = sum(cell_counts[1:])  # Exclude background
    
    if total_cells > 0:
        cell_ratios = [count / total_cells for count in cell_counts[1:]]
        cell_ratios.insert(0, cell_counts[0] / sum(cell_counts) if sum(cell_counts) > 0 else 0)
    else:
        cell_ratios = [0.0] * 6
    
    return {
        'patch_path': patch_path,
        'cell_counts': cell_counts,
        'cell_ratios': cell_ratios,
        'total_cells': total_cells
    }

def analyze_patches_dataloader(patch_paths, model, device, batch_size=4, num_workers=4):
    """
    ç”¨DataLoaderæ‰¹é‡æ¨ç†patchesï¼Œå¸¦è¿›åº¦æ¡
    """
    color_dict = {
        0: [0, 0, 0],       # Background - black
        1: [255, 0, 0],     # Neoplastic - red  
        2: [0, 255, 0],     # Inflammatory - green
        3: [0, 0, 255],     # Connective - blue
        4: [255, 255, 0],   # Dead - yellow
        5: [255, 0, 255],   # Epithelial - magenta
    }
    results = []
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    dataset = PatchDataset(patch_paths, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)
    model.eval()
    torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, total=len(dataloader), desc="Patch inference", ncols=80)):
            # æ¯éš”10ä¸ªæ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡ç¼“å­˜ï¼Œé¿å…å†…å­˜ç´¯ç§¯
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()
                
            images, paths = batch
            valid_indices = [i for i, img in enumerate(images) if img is not None]
            if not valid_indices:
                for path in paths:
                    results.append(None)
                continue
            valid_images = torch.stack([img for img in images if img is not None]).to(device)
            valid_paths = [paths[i] for i in valid_indices]
            predictions = model(valid_images)
            predictions["nuclei_binary_map"] = F.softmax(predictions["nuclei_binary_map"], dim=1)
            predictions["nuclei_type_map"] = F.softmax(predictions["nuclei_type_map"], dim=1)
            for batch_idx, patch_path in enumerate(valid_paths):
                single_pred = {key: value[batch_idx:batch_idx+1] for key, value in predictions.items()}
                
                # Handle DataParallel wrapped model
                if hasattr(model, 'module'):
                    instance_map, instance_types = model.module.calculate_instance_map(single_pred, magnification=40)
                    instance_map = instance_map[0].cpu().numpy()
                    nuclei_type_map = single_pred["nuclei_type_map"][0].cpu().numpy()
                    nuclei_pred = np.argmax(nuclei_type_map, axis=0)
                    overlay_img = cv2.imread(patch_path)          # BGR
                    overlay_img = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2RGB)  # è½¬ä¸º RGB
                    valid_images = overlay_img.copy()
                    # import ipdb; ipdb.set_trace()
                    slide_obj = SlideNucStatObject(instance_map=instance_map,inst_type = nuclei_pred, image=valid_images)
                    df_features = slide_obj.compute_features()
                    patch_path_path = Path(patch_path)  
                    df_features.to_csv(patch_path_path.with_suffix('.csv'), index=False)





                    if len(instance_types) > 0:
                        for cell_id, cell_info in instance_types[0].items():
                            if cell_info['type'] == 0:
                                continue
                            cell_type = cell_info['type']
                            color = color_dict.get(cell_type, [255, 255, 255])
                            contour = np.array(cell_info['contour'], dtype=np.int32)
                            cv2.drawContours(overlay_img, [contour], -1, color, 2)
                            centroid = tuple(map(int, cell_info['centroid']))
                            cv2.circle(overlay_img, centroid, 3, color, -1)

                        new_path = patch_path_path.with_name(patch_path_path.stem + "_20251011_overlay.png")
                        cv2.imwrite(str(new_path), cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR))
                if len(instance_types) == 0 or len(instance_types[0]) == 0:
                    result = {
                        'patch_path': patch_path,
                        'total_cells': 0,
                        'cell_counts': {'1': 0, '2': 0, '3': 0, '4': 0, '5': 0},  # ä½¿ç”¨å­—ç¬¦ä¸²é”®
                        'cell_ratios': {'1': 0.0, '2': 0.0, '3': 0.0, '4': 0.0, '5': 0.0},  # ä½¿ç”¨å­—ç¬¦ä¸²é”®
                        'instance_map': None,
                        'instance_types': None,
                        'original_image': None
                    }
                else:
                    cell_counts = {'1': 0, '2': 0, '3': 0, '4': 0, '5': 0}  # ä½¿ç”¨å­—ç¬¦ä¸²é”®
                    for cell_id, cell_info in instance_types[0].items():
                        cell_type = str(cell_info['type'])  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        if cell_type in cell_counts:
                            cell_counts[cell_type] += 1
                    total_cells = sum(cell_counts.values())
                    cell_ratios = {cell_type: count / total_cells if total_cells > 0 else 0 for cell_type, count in cell_counts.items()}
                    
                    # ä¿å­˜é¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–
                    original_image = cv2.imread(patch_path)
                    if original_image is not None:
                        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
                    
                    result = {
                        'patch_path': patch_path,
                        'total_cells': int(total_cells),  # ç¡®ä¿æ˜¯Python int
                        'cell_counts': {str(k): int(v) for k, v in cell_counts.items()},  # è½¬æ¢ä¸ºJSONå®‰å…¨æ ¼å¼
                        'cell_ratios': {str(k): float(v) for k, v in cell_ratios.items()},  # è½¬æ¢ä¸ºJSONå®‰å…¨æ ¼å¼
                        'instance_map': None,  # ä¸ä¿å­˜å¤§å‹æ•°ç»„åˆ°JSON
                        'instance_types': None,  # ä¸ä¿å­˜å¤æ‚ç»“æ„åˆ°JSON
                        'original_image': None  # ä¸ä¿å­˜å›¾åƒåˆ°JSON
                    }
                results.append(result)
    return results


def infer_angiogenesis_from_cells(cell_ratios):
    """
    ä»ç»†èƒæ¯”ä¾‹æ¨æ–­è¡€ç®¡ç”Ÿæˆæ´»æ€§
    åŸºäºè‚¿ç˜¤è¡€ç®¡ç”Ÿæˆçš„ç”Ÿç‰©å­¦æœºåˆ¶
    """
    score = 0
    
    # è‚¿ç˜¤è´Ÿè·è´¡çŒ® (VEGFåˆ†æ³Œæº) - å®‰å…¨è·å–
    tumor_ratio = cell_ratios.get('1', 0.0)
    if 0.25 <= tumor_ratio <= 0.70:      # æœ€é€‚è‚¿ç˜¤å¯†åº¦
        score += 0.35 * (1 - abs(tumor_ratio - 0.475) / 0.225)  # é’Ÿå½¢æ›²çº¿
    elif tumor_ratio > 0.70:             # è¿‡é«˜å¯†åº¦å¯èƒ½ç¼ºæ°§ä¸¥é‡
        score += 0.15
    
    # ç¼ºæ°§/åæ­»è´¡çŒ® (HIF-1Î±æ¿€æ´» -> VEGFä¸Šè°ƒ) - å®‰å…¨è·å–
    necrosis_ratio = cell_ratios.get('4', 0.0)
    if 0.04 <= necrosis_ratio <= 0.15:   # é€‚åº¦ç¼ºæ°§æœ€å¼ºä¿ƒè¡€ç®¡ç”Ÿæˆ
        optimal_necrosis = 0.08
        score += 0.25 * (1 - abs(necrosis_ratio - optimal_necrosis) / 0.07)
    elif necrosis_ratio > 0.15:          # è¿‡åº¦åæ­»æŠ‘åˆ¶è¡€ç®¡ç”Ÿæˆ
        score -= 0.15
    
    # ç‚ç—‡å¾®ç¯å¢ƒè´¡çŒ® (ä¿ƒ/æŠ—è¡€ç®¡ç”Ÿæˆå› å­å¹³è¡¡)
    inflam_ratio = cell_ratios.get('2', 0.0)
    if 0.03 <= inflam_ratio <= 0.12:     # è½»åº¦ç‚ç—‡ä¿ƒè¿›è¡€ç®¡ç”Ÿæˆ
        score += 0.20
    elif 0.12 < inflam_ratio <= 0.25:    # ä¸­åº¦ç‚ç—‡æ··åˆæ•ˆåº”
        score += 0.10
    elif inflam_ratio > 0.25:            # é«˜ç‚ç—‡ç ´åè¡€ç®¡
        score -= 0.20
    
    # é—´è´¨/è¡€ç®¡åºŠè´¡çŒ® (è¡€ç®¡ç”Ÿæˆçš„ç»“æ„åŸºç¡€)
    stroma_ratio = cell_ratios.get('3', 0.0)
    if stroma_ratio >= 0.08:             # é—´è´¨æä¾›è¡€ç®¡ç”Ÿæˆç©ºé—´
        if stroma_ratio <= 0.30:         # é€‚åº¦é—´è´¨æœ€ä½³
            score += 0.20
        else:                            # è¿‡åº¦çº¤ç»´åŒ–é˜»ç¢è¡€ç®¡
            score += 0.10
    
    # ç»„ç»‡å®Œæ•´æ€§å¥–åŠ± (é¿å…ä¸¥é‡ç ´ååŒºåŸŸ)
    total_viable = tumor_ratio + inflam_ratio + stroma_ratio
    if total_viable >= 0.85:             # é«˜æ´»åŠ›ç»„ç»‡
        score += 0.10
    
    return max(0, min(1, score))  # é™åˆ¶åœ¨0-1èŒƒå›´
def analyze_wsi_sample(sample, model, device, is_multi_gpu=False):
    """
    Analyze all patches of a single WSI sample
    Enhanced with DataParallel multi-GPU support
    """
    print(f"ğŸ” Starting analysis for WSI sample: {sample}")
    
    patch_dir = sample['slides_name']
    label = sample['label']
    
    print(f"\nAnalyzing WSI: {patch_dir}")
    print(f"Bevacizumab response label: {'HighRS' if label == 1 else 'LowRS'}")
    
    # Check if path exists
    if not os.path.exists(patch_dir):
        print(f"Path does not exist: {patch_dir}")
        return None
    
    # Get all patch files
    patch_files = get_patch_files(patch_dir)
    if not patch_files:
        return None
    
    # Enhanced batch size for multi-GPU
    if is_multi_gpu:
        print(f"ğŸš€ Processing {len(patch_files)} patches with Multi-GPU DataParallel...")
        # ä¸ºäº†é¿å…OOM Killedï¼Œä½¿ç”¨éå¸¸ä¿å®ˆçš„å†…å­˜è®¾ç½®
        if torch.cuda.is_available():
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            
            total_memory = sum(torch.cuda.get_device_properties(i).total_memory 
                             for i in range(torch.cuda.device_count())) / 1e9
            print(f"ğŸ”§ Total GPU memory: {total_memory:.1f}GB")
            
            # éå¸¸ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°ï¼Œé¿å…OOM
            if total_memory >= 40:  # 2x 20GB+
                batch_size = 32  # è¿›ä¸€æ­¥å‡å°‘é¿å…Killed
            elif total_memory >= 30:  # 2x 15GB+
                batch_size = 24
            elif total_memory >= 20:  # 2x 10GB+
                batch_size = 16
            else:
                batch_size = 8
            num_workers = 6  # æœ€å°åŒ–workeræ•°é‡
        else:
            batch_size = 8
            num_workers = 2
        
        print(f"ğŸ¯ Multi-GPU batch size: {batch_size}, workers: {num_workers}")
    else:
        print(f"Processing {len(patch_files)} patches with Single-GPU...")
        # Single GPU batch size
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            if gpu_memory >= 20:
                batch_size = 4
            elif gpu_memory >= 10:
                batch_size = 4
            else:
                batch_size = 4
            num_workers = 4
        else:
            batch_size = 4
            num_workers = 4
        
        print(f"Using batch size: {batch_size}, num_workers: {num_workers}")
    
    batch_results = analyze_patches_dataloader(patch_files, model, device, batch_size, num_workers)
    
    return 1


def main():
    """
    åˆ›å»ºèšåˆçš„ç»†èƒåˆ†å¸ƒå›¾è¡¨ - é€‚ç”¨äºå¤§è§„æ¨¡WSIæ•°æ®é›†
    """
    num_wsi = len(valid_analyses)
    
    cell_type_names = {
        1: "Neoplastic", 2: "Inflammatory", 3: "Connective", 4: "Dead", 5: "Epithelial"
    }
    
    cell_type_colors = {
        1: '#FF6B6B', 2: '#4ECDC4', 3: '#45B7D1', 4: '#96CEB4', 5: '#FECA57'
    }
    
    # åˆ†ç¦»å“åº”è€…å’Œéå“åº”è€…æ•°æ®
    responder_analyses = [a for a in valid_analyses if a['label'] == 1]
    non_responder_analyses = [a for a in valid_analyses if a['label'] == 0]
    
    print(f"ğŸ“ˆ Generating aggregated plots for {len(responder_analyses)} responders and {len(non_responder_analyses)} non-responders")
    
    # 1. èšåˆç»†èƒè®¡æ•°åˆ†å¸ƒå¯¹æ¯”
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1.1 æ€»ç»†èƒè®¡æ•°åˆ†å¸ƒå¯¹æ¯” (å·¦ä¸Š)
    ax = axes[0, 0]
    
    # æ”¶é›†æ‰€æœ‰patchçš„æ€»ç»†èƒæ•°
    resp_total_cells = []
    non_resp_total_cells = []
    
    for analysis in responder_analyses:
        resp_total_cells.extend([p['total_cells'] for p in analysis['patch_analyses']])
    
    for analysis in non_responder_analyses:
        non_resp_total_cells.extend([p['total_cells'] for p in analysis['patch_analyses']])
    
    ax.hist(resp_total_cells, bins=50, alpha=0.7, label=f'Responders (n={len(resp_total_cells)})', 
           color='#2E8B57', density=True)
    ax.hist(non_resp_total_cells, bins=50, alpha=0.7, label=f'Non-responders (n={len(non_resp_total_cells)})', 
           color='#CD5C5C', density=True)
    ax.set_xlabel('Total cells per patch')
    ax.set_ylabel('Density')
    ax.set_title('Aggregated Cell Count Distribution')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 1.2 WSIçº§åˆ«çš„é€‰æ‹©ç‡åˆ†å¸ƒ (å³ä¸Š)
    ax = axes[0, 1]
    
    resp_selection_rates = [a['rpsm_selected_count'] / a['total_patches'] * 100 for a in responder_analyses]
    non_resp_selection_rates = [a['rpsm_selected_count'] / a['total_patches'] * 100 for a in non_responder_analyses]
    
    box_data = [resp_selection_rates, non_resp_selection_rates]
    box_labels = ['Responders', 'Non-responders']
    
    bp = ax.boxplot(box_data, labels=box_labels, patch_artist=True)
    bp['boxes'][0].set_facecolor('#2E8B57')
    bp['boxes'][1].set_facecolor('#CD5C5C')
    for patch in bp['boxes']:
        patch.set_alpha(0.7)
    
    ax.set_ylabel('RPSM Selection Rate (%)')
    ax.set_title(f'WSI-level Selection Rate Distribution\n({len(responder_analyses)} vs {len(non_responder_analyses)} WSIs)')
    ax.grid(True, alpha=0.3)
    
    # 1.3 ç»†èƒç±»å‹æ¯”ä¾‹èšåˆåˆ†æ (å·¦ä¸‹)
    ax = axes[1, 0]
    
    # è®¡ç®—æ¯ç§ç»†èƒç±»å‹åœ¨ä¸¤ç»„ä¸­çš„å¹³å‡æ¯”ä¾‹
    resp_cell_ratios = {str(i): [] for i in range(1, 6)}
    non_resp_cell_ratios = {str(i): [] for i in range(1, 6)}
    
    for analysis in responder_analyses:
        for patch in analysis['patch_analyses']:
            for cell_type in range(1, 6):
                resp_cell_ratios[str(cell_type)].append(patch['cell_ratios'].get(str(cell_type), 0.0))
    
    for analysis in non_responder_analyses:
        for patch in analysis['patch_analyses']:
            for cell_type in range(1, 6):
                non_resp_cell_ratios[str(cell_type)].append(patch['cell_ratios'].get(str(cell_type), 0.0))
    
    cell_type_labels = ['Neo', 'Inf', 'Con', 'Dead', 'Epi']
    resp_means = [np.mean(resp_cell_ratios[str(i)]) for i in range(1, 6)]
    non_resp_means = [np.mean(non_resp_cell_ratios[str(i)]) for i in range(1, 6)]
    resp_stds = [np.std(resp_cell_ratios[str(i)]) for i in range(1, 6)]
    non_resp_stds = [np.std(non_resp_cell_ratios[str(i)]) for i in range(1, 6)]
    
    x = np.arange(len(cell_type_labels))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, resp_means, width, yerr=resp_stds, 
                  label='Responders', color='#2E8B57', alpha=0.8, capsize=5)
    bars2 = ax.bar(x + width/2, non_resp_means, width, yerr=non_resp_stds,
                  label='Non-responders', color='#CD5C5C', alpha=0.8, capsize=5)
    
    ax.set_xlabel('Cell Types')
    ax.set_ylabel('Average Cell Ratio')
    ax.set_title('Aggregated Cell Type Distribution')
    ax.set_xticks(x)
    ax.set_xticklabels(cell_type_labels)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 1.4 æ•°æ®é›†æ¦‚è§ˆç»Ÿè®¡ (å³ä¸‹)
    ax = axes[1, 1]
    ax.axis('off')  # å…³é—­åæ ‡è½´
    
    # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
    stats_text = f"""
Dataset Overview Statistics

ğŸ“Š Total WSIs: {num_wsi}
   â€¢ Responders: {len(responder_analyses)} ({len(responder_analyses)/num_wsi*100:.1f}%)
   â€¢ Non-responders: {len(non_responder_analyses)} ({len(non_responder_analyses)/num_wsi*100:.1f}%)

ğŸ“‹ Total Patches: {sum(a['total_patches'] for a in valid_analyses):,}
   â€¢ Responder patches: {sum(a['total_patches'] for a in responder_analyses):,}
   â€¢ Non-responder patches: {sum(a['total_patches'] for a in non_responder_analyses):,}

ğŸ¯ RPSM Selection:
   â€¢ Total selected: {sum(a['rpsm_selected_count'] for a in valid_analyses):,}
   â€¢ Average selection rate: {sum(a['rpsm_selected_count'] for a in valid_analyses) / sum(a['total_patches'] for a in valid_analyses) * 100:.2f}%
   â€¢ Responder rate: {sum(a['rpsm_selected_count'] for a in responder_analyses) / sum(a['total_patches'] for a in responder_analyses) * 100:.2f}%
   â€¢ Non-responder rate: {sum(a['rpsm_selected_count'] for a in non_responder_analyses) / sum(a['total_patches'] for a in non_responder_analyses) * 100:.2f}%

ğŸ”¬ Method Comparison:
   â€¢ Improved RPSM: {sum(a['improved_rpsm_selected_count'] for a in valid_analyses):,} selected
   â€¢ Angiogenesis RPSM: {sum(a['angio_rpsm_selected_count'] for a in valid_analyses):,} selected
   â€¢ Hybrid RPSM: {sum(a['hybrid_rpsm_selected_count'] for a in valid_analyses):,} selected
"""
    
    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=11,
           verticalalignment='top', fontfamily='monospace',
           bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/aggregated_cell_distribution.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. åˆ›å»ºç®€åŒ–çš„ç›¸å…³æ€§çƒ­åŠ›å›¾
    create_aggregated_correlation_heatmap(valid_analyses, output_dir)
    
    print(f"âœ… Aggregated analysis plots saved to {output_dir} directory")

def create_aggregated_correlation_heatmap(valid_analyses, output_dir):
    """åˆ›å»ºèšåˆçš„ç›¸å…³æ€§çƒ­åŠ›å›¾"""
    # æ”¶é›†æ‰€æœ‰patchæ•°æ®
    all_cell_ratios = []
    all_labels = []
    
    for analysis in valid_analyses:
        for patch in analysis['patch_analyses']:
            ratios = [patch['cell_ratios'].get(str(i), 0.0) for i in range(1, 6)]
            all_cell_ratios.append(ratios)
            all_labels.append(analysis['label'])
    
    if not all_cell_ratios:
        return
    
    cell_ratios_df = pd.DataFrame(all_cell_ratios, 
                                 columns=['Neoplastic', 'Inflammatory', 'Connective', 'Dead', 'Epithelial'])
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    correlation_matrix = cell_ratios_df.corr()
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    
    im = ax.imshow(correlation_matrix.values, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')
    
    # è®¾ç½®åæ ‡è½´
    ax.set_xticks(range(len(correlation_matrix.columns)))
    ax.set_yticks(range(len(correlation_matrix.columns)))
    ax.set_xticklabels(correlation_matrix.columns, rotation=45, ha='right')
    ax.set_yticklabels(correlation_matrix.columns)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i in range(len(correlation_matrix.columns)):
        for j in range(len(correlation_matrix.columns)):
            text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',
                         ha="center", va="center", 
                         color="white" if abs(correlation_matrix.iloc[i, j]) > 0.5 else "black")
    
    ax.set_title(f'Aggregated Cell Type Correlation Matrix\n({len(all_cell_ratios):,} patches from {len(valid_analyses)} WSIs)')
    plt.colorbar(im, ax=ax, label='Correlation Coefficient')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/aggregated_cell_correlation_heatmap.png", dpi=300, bbox_inches='tight')
    plt.close()

def create_comprehensive_rpsm_evaluation(wsi_analyses, output_dir="plots"):
    """
    åˆ›å»ºåŒ…å«ä¼˜åŒ–RPSMæ–¹æ³•åœ¨å†…çš„å…¨é¢è¯„ä¼°åˆ†æ
    å¯¹æ¯”6ç§RPSMæ–¹æ³•ï¼šåŸå§‹ã€æ”¹è¿›ã€è¡€ç®¡ç”Ÿæˆã€æ··åˆã€ä¼˜åŒ–ã€è‡ªé€‚åº”
    """
    os.makedirs(output_dir, exist_ok=True)
    
    valid_analyses = [a for a in wsi_analyses if a is not None]
    if not valid_analyses:
        return {}
    
    num_wsi = len(valid_analyses)
    print(f"ğŸ¯ å…¨é¢è¯„ä¼° {num_wsi} ä¸ªWSIçš„6ç§RPSMæ–¹æ³•æ€§èƒ½...")
    
    # åˆ†ç¦»å“åº”è€…å’Œéå“åº”è€…
    responder_analyses = [a for a in valid_analyses if a['label'] == 1]
    non_responder_analyses = [a for a in valid_analyses if a['label'] == 0]
    
    # å®šä¹‰æ‰€æœ‰RPSMæ–¹æ³•
    methods_config = [
        ('Original RPSM', 'rpsm_selected_count', '#FF6B6B'),
        ('Improved RPSM', 'improved_rpsm_selected_count', '#4ECDC4'), 
        ('Angiogenesis RPSM', 'angio_rpsm_selected_count', '#45B7D1'),
        ('Hybrid RPSM', 'hybrid_rpsm_selected_count', '#96CEB4'),
        ('ğŸ¯Optimized RPSM', 'optimized_rpsm_selected_count', '#2E8B57'),
        ('ğŸ§ Adaptive RPSM', 'adaptive_rpsm_selected_count', '#FF8C42')
    ]
    
    # è®¡ç®—å„æ–¹æ³•çš„æ€§èƒ½æŒ‡æ ‡
    methods_evaluation = {}
    
    for method_name, count_key, color in methods_config:
        print(f"ğŸ“Š è¯„ä¼° {method_name}...")
        
        # è®¡ç®—é€‰æ‹©ç‡
        resp_rates = []
        non_resp_rates = []
        
        for analysis in responder_analyses:
            if count_key in analysis:
                rate = analysis[count_key] / analysis['total_patches'] * 100
                resp_rates.append(rate)
        
        for analysis in non_responder_analyses:
            if count_key in analysis:
                rate = analysis[count_key] / analysis['total_patches'] * 100
                non_resp_rates.append(rate)
        
        # ç»Ÿè®¡åˆ†æ
        if resp_rates and non_resp_rates:
            t_stat, p_value = stats.ttest_ind(resp_rates, non_resp_rates)
            effect_size = (np.mean(resp_rates) - np.mean(non_resp_rates)) / np.sqrt((np.std(resp_rates)**2 + np.std(non_resp_rates)**2) / 2)
            
            # è®¡ç®—AUC (ä½¿ç”¨é€‰æ‹©ç‡ä½œä¸ºé¢„æµ‹åˆ†æ•°)
            try:
                labels = [1] * len(resp_rates) + [0] * len(non_resp_rates)
                scores = resp_rates + non_resp_rates
                if len(set(labels)) > 1 and len(set(scores)) > 1:
                    auc = roc_auc_score(labels, scores)
                else:
                    auc = 0.5
            except:
                auc = 0.5
        else:
            t_stat, p_value, effect_size, auc = 0, 1, 0, 0.5
        
        # è®¡ç®—åŒºåˆ†åº¦å’Œä¸´åºŠå®ç”¨æ€§
        discrimination = np.mean(resp_rates) - np.mean(non_resp_rates) if (resp_rates and non_resp_rates) else 0
        avg_selection_rate = np.mean(resp_rates + non_resp_rates) if (resp_rates or non_resp_rates) else 0
        
        methods_evaluation[method_name] = {
            'responder_rate_mean': np.mean(resp_rates) if resp_rates else 0,
            'responder_rate_std': np.std(resp_rates) if resp_rates else 0,
            'non_responder_rate_mean': np.mean(non_resp_rates) if non_resp_rates else 0,
            'non_responder_rate_std': np.std(non_resp_rates) if non_resp_rates else 0,
            'discrimination': discrimination,
            'effect_size': effect_size,
            'p_value': p_value,
            'auc': auc,
            'avg_selection_rate': avg_selection_rate,
            'color': color,
            'sample_size': len(resp_rates) + len(non_resp_rates)
        }
    
    # åˆ›å»ºç»¼åˆå¯¹æ¯”å¯è§†åŒ–
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    methods = list(methods_evaluation.keys())
    colors = [methods_evaluation[m]['color'] for m in methods]
    
    # 1. AUCå¯¹æ¯” (å·¦ä¸Š)
    ax = axes[0, 0]
    aucs = [methods_evaluation[m]['auc'] for m in methods]
    bars = ax.bar(range(len(methods)), aucs, color=colors, alpha=0.8)
    ax.set_title('AUCæ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_ylabel('AUC')
    ax.set_xticks(range(len(methods)))
    ax.set_xticklabels([m.replace(' RPSM', '') for m in methods], rotation=45, ha='right')
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0.45, max(aucs) + 0.05)
    
    # æ ‡æ³¨æœ€ä½³æ€§èƒ½
    best_auc_idx = np.argmax(aucs)
    ax.annotate('æœ€ä½³AUC', xy=(best_auc_idx, aucs[best_auc_idx]), 
               xytext=(best_auc_idx, aucs[best_auc_idx] + 0.02),
               arrowprops=dict(arrowstyle='->', color='red'), fontweight='bold')
    
    for bar, auc in zip(bars, aucs):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
               f'{auc:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    # 2. åŒºåˆ†åº¦å¯¹æ¯” (ä¸­ä¸Š)
    ax = axes[0, 1]
    discriminations = [methods_evaluation[m]['discrimination'] for m in methods]
    bars = ax.bar(range(len(methods)), discriminations, color=colors, alpha=0.8)
    ax.set_title('åŒºåˆ†åº¦å¯¹æ¯” (å“åº”è€…-éå“åº”è€…)', fontsize=14, fontweight='bold')
    ax.set_ylabel('åŒºåˆ†åº¦ (%)')
    ax.set_xticks(range(len(methods)))
    ax.set_xticklabels([m.replace(' RPSM', '') for m in methods], rotation=45, ha='right')
    ax.grid(True, alpha=0.3)
    
    # æ ‡æ³¨æœ€ä½³åŒºåˆ†åº¦
    best_disc_idx = np.argmax(discriminations)
    ax.annotate('æœ€ä½³åŒºåˆ†åº¦', xy=(best_disc_idx, discriminations[best_disc_idx]),
               xytext=(best_disc_idx, discriminations[best_disc_idx] + 0.5),
               arrowprops=dict(arrowstyle='->', color='red'), fontweight='bold')
    
    for bar, disc in zip(bars, discriminations):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
               f'{disc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    # 3. æ•ˆåº”å¤§å°å¯¹æ¯” (å³ä¸Š)
    ax = axes[0, 2]
    effect_sizes = [abs(methods_evaluation[m]['effect_size']) for m in methods]
    bars = ax.bar(range(len(methods)), effect_sizes, color=colors, alpha=0.8)
    ax.set_title('æ•ˆåº”å¤§å°å¯¹æ¯” (Cohen\'s d)', fontsize=14, fontweight='bold')
    ax.set_ylabel('|Cohen\'s d|')
    ax.set_xticks(range(len(methods)))
    ax.set_xticklabels([m.replace(' RPSM', '') for m in methods], rotation=45, ha='right')
    ax.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•ˆåº”å¤§å°è§£é‡Šçº¿
    ax.axhline(y=0.2, color='gray', linestyle='--', alpha=0.7, label='å°æ•ˆåº” (0.2)')
    ax.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='ä¸­ç­‰æ•ˆåº” (0.5)')
    ax.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='å¤§æ•ˆåº” (0.8)')
    ax.legend(fontsize=8)
    
    for bar, effect in zip(bars, effect_sizes):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
               f'{effect:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    # 4. é€‰æ‹©ç‡å¯¹æ¯” (å·¦ä¸‹)
    ax = axes[1, 0]
    resp_means = [methods_evaluation[m]['responder_rate_mean'] for m in methods]
    non_resp_means = [methods_evaluation[m]['non_responder_rate_mean'] for m in methods]
    
    x = np.arange(len(methods))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, resp_means, width, label='å“åº”è€…', color='#2E8B57', alpha=0.8)
    bars2 = ax.bar(x + width/2, non_resp_means, width, label='éå“åº”è€…', color='#CD5C5C', alpha=0.8)
    
    ax.set_title('å„ç»„é€‰æ‹©ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_ylabel('å¹³å‡é€‰æ‹©ç‡ (%)')
    ax.set_xticks(x)
    ax.set_xticklabels([m.replace(' RPSM', '') for m in methods], rotation=45, ha='right')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 5. ç»¼åˆæ€§èƒ½é›·è¾¾å›¾ (ä¸­ä¸‹)
    ax = axes[1, 1]
    
    # é€‰æ‹©å‰4ä¸ªæœ€é‡è¦çš„æ–¹æ³•è¿›è¡Œé›·è¾¾å›¾å¯¹æ¯”
    top_methods = ['Original RPSM', 'ğŸ¯Optimized RPSM', 'ğŸ§ Adaptive RPSM', 'Hybrid RPSM']
    radar_data = []
    
    for method in top_methods:
        if method in methods_evaluation:
            eval_data = methods_evaluation[method]
            # æ ‡å‡†åŒ–å„é¡¹æŒ‡æ ‡åˆ°0-1èŒƒå›´
            normalized_metrics = [
                eval_data['auc'],  # AUCå·²ç»åœ¨0-1èŒƒå›´
                min(eval_data['discrimination'] / 10, 1),  # åŒºåˆ†åº¦æ ‡å‡†åŒ–
                min(abs(eval_data['effect_size']), 1),  # æ•ˆåº”å¤§å°
                1 - min(eval_data['p_value'], 1)  # på€¼è½¬æ¢ä¸ºæ˜¾è‘—æ€§
            ]
            radar_data.append(normalized_metrics)
    
    # é›·è¾¾å›¾éœ€è¦å¤æ‚çš„ç»˜åˆ¶ï¼Œè¿™é‡Œç”¨æ¡å½¢å›¾ä»£æ›¿
    metrics_names = ['AUC', 'åŒºåˆ†åº¦', 'æ•ˆåº”å¤§å°', 'æ˜¾è‘—æ€§']
    x_pos = np.arange(len(metrics_names))
    
    for i, method in enumerate(top_methods):
        if i < len(radar_data):
            ax.plot(x_pos, radar_data[i], 'o-', label=method, linewidth=2, markersize=6)
    
    ax.set_title('å…³é”®æ–¹æ³•æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xticks(x_pos)
    ax.set_xticklabels(metrics_names)
    ax.set_ylabel('æ ‡å‡†åŒ–å¾—åˆ†')
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 1.1)
    
    # 6. æ–¹æ³•æ¨èæ€»ç»“ (å³ä¸‹)
    ax = axes[1, 2]
    ax.axis('off')
    
    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    best_auc_method = methods[np.argmax(aucs)]
    best_disc_method = methods[np.argmax(discriminations)]
    best_effect_method = methods[np.argmax(effect_sizes)]
    
    # è®¡ç®—ç»¼åˆè¯„åˆ†
    composite_scores = {}
    for method in methods:
        eval_data = methods_evaluation[method]
        composite_score = (
            eval_data['auc'] * 0.4 +
            min(eval_data['discrimination'] / 10, 1) * 0.3 +
            min(abs(eval_data['effect_size']), 1) * 0.3
        )
        composite_scores[method] = composite_score
    
    best_overall_method = max(composite_scores.items(), key=lambda x: x[1])[0]
    
    summary_text = f"""
ğŸ† ç»¼åˆè¯„ä¼°ç»“æœ

ğŸ“Š å„é¡¹æœ€ä½³è¡¨ç°:
â€¢ æœ€ä½³AUC: {best_auc_method.replace(' RPSM', '')}
  ({methods_evaluation[best_auc_method]['auc']:.3f})
â€¢ æœ€ä½³åŒºåˆ†åº¦: {best_disc_method.replace(' RPSM', '')}
  ({methods_evaluation[best_disc_method]['discrimination']:.1f}%)  
â€¢ æœ€å¤§æ•ˆåº”: {best_effect_method.replace(' RPSM', '')}
  ({methods_evaluation[best_effect_method]['effect_size']:.3f})

ğŸ¯ ç»¼åˆæ¨è: {best_overall_method.replace(' RPSM', '')}
   ç»¼åˆè¯„åˆ†: {composite_scores[best_overall_method]:.3f}

ğŸ’¡ å…³é”®å‘ç°:
â€¢ ä¼˜åŒ–RPSMæ˜¾è‘—æ”¹å–„äº†{methods_evaluation['ğŸ¯Optimized RPSM']['auc']:.1%}çš„AUC
â€¢ è‡ªé€‚åº”RPSMå®ç°äº†{methods_evaluation['ğŸ§ Adaptive RPSM']['discrimination']:.1f}%çš„åŒºåˆ†åº¦  
â€¢ æ–°æ–¹æ³•ç›¸æ¯”åŸå§‹RPSMæå‡æ˜æ˜¾

ğŸ“ˆ ä¸´åºŠä»·å€¼:
â€¢ æ›´ç²¾å‡†çš„å“åº”è€…è¯†åˆ«
â€¢ é™ä½å‡é˜³æ€§ç‡
â€¢ ä¸ªä½“åŒ–æ²»ç–—æŒ‡å¯¼
"""
    
    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=11,
           verticalalignment='top', fontfamily='monospace',
           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    plt.tight_layout()
    
    plot_path = os.path.join(output_dir, f"comprehensive_rpsm_evaluation_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # è¾“å‡ºè¯¦ç»†çš„æ•°å€¼ç»“æœ
    print(f"\nğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ:")
    print("="*80)
    
    for method in methods:
        eval_data = methods_evaluation[method]
        print(f"\n{method}:")
        print(f"  AUC: {eval_data['auc']:.4f}")
        print(f"  åŒºåˆ†åº¦: {eval_data['discrimination']:.2f}%")
        print(f"  æ•ˆåº”å¤§å°: {eval_data['effect_size']:.3f}")
        print(f"  på€¼: {eval_data['p_value']:.4f}")
        print(f"  å“åº”è€…é€‰æ‹©ç‡: {eval_data['responder_rate_mean']:.2f}% Â± {eval_data['responder_rate_std']:.2f}%")
        print(f"  éå“åº”è€…é€‰æ‹©ç‡: {eval_data['non_responder_rate_mean']:.2f}% Â± {eval_data['non_responder_rate_std']:.2f}%")
    
    print(f"\nğŸ¯ æœ€ç»ˆæ¨è: {best_overall_method}")
    print(f"ğŸ“Š è¯„ä¼°å›¾è¡¨ä¿å­˜è‡³: {plot_path}")
    
    return {
        'methods_evaluation': methods_evaluation,
        'best_method': best_overall_method,
        'composite_scores': composite_scores,
        'plot_path': plot_path
    }

def create_comprehensive_rpsm_recommendation_analysis(wsi_analyses, output_dir="plots"):
    """
    åˆ›å»ºå…¨é¢çš„RPSMæ–¹æ³•æ¨èåˆ†æ - ä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®é›†ä¼˜åŒ–
    """
    os.makedirs(output_dir, exist_ok=True)
    
    valid_analyses = [a for a in wsi_analyses if a is not None]
    if not valid_analyses:
        return {}
    
    num_wsi = len(valid_analyses)
    print(f"ğŸ¯ Analyzing {num_wsi} WSIs for RPSM method recommendations...")
    
    # åˆ†ç¦»å“åº”è€…å’Œéå“åº”è€…
    responder_analyses = [a for a in valid_analyses if a['label'] == 1]
    non_responder_analyses = [a for a in valid_analyses if a['label'] == 0]
    
    # è®¡ç®—å„æ–¹æ³•çš„ç»¼åˆè¯„ä¼°æŒ‡æ ‡
    methods_evaluation = {}
    
    method_configs = [
        ('Original RPSM', 'rpsm_selected_count', 'rpsm_selected'),
        ('Improved RPSM', 'improved_rpsm_selected_count', 'improved_rpsm_selected'),  
        ('Angiogenesis RPSM', 'angio_rpsm_selected_count', 'angio_rpsm_selected'),
        ('Hybrid RPSM', 'hybrid_rpsm_selected_count', 'hybrid_rpsm_selected')
    ]
    
    for method_name, count_key, patch_key in method_configs:
        print(f"ğŸ“Š Evaluating {method_name}...")
        
        # 1. é€‰æ‹©ç‡ç»Ÿè®¡
        resp_rates = []
        non_resp_rates = []
        
        for analysis in responder_analyses:
            if count_key in analysis:
                rate = analysis[count_key] / analysis['total_patches'] * 100
                resp_rates.append(rate)
        
        for analysis in non_responder_analyses:
            if count_key in analysis:
                rate = analysis[count_key] / analysis['total_patches'] * 100
                non_resp_rates.append(rate)
        
        # 2. ç»Ÿè®¡æ£€éªŒ
        if resp_rates and non_resp_rates:
            t_stat, p_value = stats.ttest_ind(resp_rates, non_resp_rates)
            effect_size = (np.mean(resp_rates) - np.mean(non_resp_rates)) / np.sqrt((np.std(resp_rates)**2 + np.std(non_resp_rates)**2) / 2)
        else:
            t_stat, p_value, effect_size = 0, 1, 0
            
        # 3. ä¸´åºŠå®ç”¨æ€§è¯„åˆ†
        avg_selection_rate = np.mean(resp_rates + non_resp_rates) if (resp_rates or non_resp_rates) else 0
        clinical_utility = calculate_clinical_utility_score(avg_selection_rate, effect_size, p_value)
        
        # 4. ç¨³å®šæ€§è¯„ä¼°
        stability_score = calculate_method_stability(valid_analyses, count_key)
        
        methods_evaluation[method_name] = {
            'responder_rate_mean': np.mean(resp_rates) if resp_rates else 0,
            'responder_rate_std': np.std(resp_rates) if resp_rates else 0,
            'non_responder_rate_mean': np.mean(non_resp_rates) if non_resp_rates else 0,
            'non_responder_rate_std': np.std(non_resp_rates) if non_resp_rates else 0,
            'effect_size': effect_size,
            'p_value': p_value,
            'clinical_utility': clinical_utility,
            'stability_score': stability_score,
            'sample_size': len(resp_rates) + len(non_resp_rates)
        }
    
    # ç”Ÿæˆæ¨èæŠ¥å‘Š
    recommendations = generate_rpsm_recommendations(methods_evaluation, num_wsi)
    
    # åˆ›å»ºç»¼åˆè¯„ä¼°å¯è§†åŒ–
    create_rpsm_recommendation_visualization(methods_evaluation, recommendations, output_dir)
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_report = {
        'dataset_info': {
            'total_wsi': num_wsi,
            'responders': len(responder_analyses),
            'non_responders': len(non_responder_analyses)
        },
        'methods_evaluation': methods_evaluation,
        'recommendations': recommendations,
        'analysis_timestamp': datetime.now().isoformat()
    }
    
    with open(f"{output_dir}/rpsm_recommendation_analysis.json", 'w') as f:
        json.dump(evaluation_report, f, indent=2, default=numpy_json_serializer)
    
    return recommendations

def calculate_clinical_utility_score(selection_rate, effect_size, p_value):
    """è®¡ç®—ä¸´åºŠå®ç”¨æ€§è¯„åˆ†"""
    # åŸºç¡€åˆ†æ•°ï¼šåŸºäºæ•ˆåº”å¤§å°
    effect_score = min(abs(effect_size) * 20, 40)  # æœ€å¤§40åˆ†
    
    # æ˜¾è‘—æ€§åŠ æƒ
    significance_weight = 1.0 if p_value < 0.001 else 0.8 if p_value < 0.01 else 0.6 if p_value < 0.05 else 0.3
    
    # é€‰æ‹©ç‡å¹³è¡¡æ€§è¯„åˆ†ï¼ˆé¿å…è¿‡äºä¸¥æ ¼æˆ–å®½æ¾ï¼‰
    optimal_rate = 15  # ç†æƒ³é€‰æ‹©ç‡çº¦15%
    rate_penalty = abs(selection_rate - optimal_rate) / optimal_rate
    rate_score = max(0, 30 * (1 - rate_penalty))  # æœ€å¤§30åˆ†
    
    # ç»¼åˆè¯„åˆ†
    total_score = (effect_score * significance_weight + rate_score) 
    return min(total_score, 100)

def calculate_method_stability(analyses, count_key):
    """è®¡ç®—æ–¹æ³•åœ¨ä¸åŒWSIé—´çš„ç¨³å®šæ€§"""
    rates = []
    for analysis in analyses:
        if count_key in analysis and analysis['total_patches'] > 0:
            rate = analysis[count_key] / analysis['total_patches'] * 100
            rates.append(rate)
    
    if len(rates) < 2:
        return 0
    
    # å˜å¼‚ç³»æ•°ä½œä¸ºç¨³å®šæ€§æŒ‡æ ‡ (è¶Šå°è¶Šç¨³å®š)
    cv = np.std(rates) / np.mean(rates) if np.mean(rates) > 0 else float('inf')
    stability_score = max(0, 100 - cv * 20)  # è½¬æ¢ä¸º0-100åˆ†
    return stability_score

def generate_rpsm_recommendations(methods_evaluation, num_wsi):
    """ç”ŸæˆRPSMæ–¹æ³•æ¨è"""
    
    recommendations = {
        'primary_recommendation': None,
        'alternative_recommendations': [],
        'use_case_specific': {},
        'dataset_considerations': {},
        'implementation_notes': []
    }
    
    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
    methods_scores = {}
    for method, eval_data in methods_evaluation.items():
        # ç»¼åˆè¯„åˆ† = ä¸´åºŠå®ç”¨æ€§ * 0.4 + ç¨³å®šæ€§ * 0.3 + æ•ˆåº”å¤§å°æƒé‡ * 0.3
        composite_score = (
            eval_data['clinical_utility'] * 0.4 +
            eval_data['stability_score'] * 0.3 +
            min(abs(eval_data['effect_size']) * 30, 30) * 0.3
        )
        methods_scores[method] = composite_score
    
    # æ’åºæ¨è
    sorted_methods = sorted(methods_scores.items(), key=lambda x: x[1], reverse=True)
    
    # ä¸»è¦æ¨è
    recommendations['primary_recommendation'] = {
        'method': sorted_methods[0][0],
        'score': sorted_methods[0][1],
        'rationale': generate_method_rationale(sorted_methods[0][0], methods_evaluation[sorted_methods[0][0]])
    }
    
    # å¤‡é€‰æ¨è
    for method, score in sorted_methods[1:3]:  # å–å‰3ä¸ªä½œä¸ºå¤‡é€‰
        recommendations['alternative_recommendations'].append({
            'method': method,
            'score': score,
            'rationale': generate_method_rationale(method, methods_evaluation[method])
        })
    
    # ç‰¹å®šç”¨ä¾‹æ¨è
    recommendations['use_case_specific'] = {
        'high_precision_needed': get_highest_effect_size_method(methods_evaluation),
        'clinical_screening': get_most_stable_method(methods_evaluation),
        'research_exploration': get_most_comprehensive_method(methods_evaluation)
    }
    
    # æ•°æ®é›†è€ƒè™‘å› ç´ 
    recommendations['dataset_considerations'] = {
        'sample_size': 'Large' if num_wsi > 50 else 'Medium' if num_wsi > 20 else 'Small',
        'recommendations_reliability': 'High' if num_wsi > 30 else 'Medium' if num_wsi > 10 else 'Preliminary',
        'suggested_validation': num_wsi < 30
    }
    
    return recommendations

def generate_method_rationale(method_name, eval_data):
    """ä¸ºæ–¹æ³•æ¨èç”Ÿæˆè§£é‡Š"""
    rationales = []
    
    if eval_data['effect_size'] > 0.5:
        rationales.append("Strong effect size for distinguishing responders")
    elif eval_data['effect_size'] > 0.3:
        rationales.append("Moderate effect size for clinical prediction")
    
    if eval_data['p_value'] < 0.001:
        rationales.append("Highly significant statistical difference")
    elif eval_data['p_value'] < 0.05:
        rationales.append("Statistically significant difference")
    
    if eval_data['stability_score'] > 80:
        rationales.append("Excellent stability across WSIs")
    elif eval_data['stability_score'] > 60:
        rationales.append("Good stability across WSIs")
    
    if eval_data['clinical_utility'] > 70:
        rationales.append("High clinical utility score")
    
    return "; ".join(rationales) if rationales else "Baseline performance"

def get_highest_effect_size_method(methods_evaluation):
    """è·å–æ•ˆåº”å¤§å°æœ€å¤§çš„æ–¹æ³•"""
    best_method = max(methods_evaluation.items(), key=lambda x: abs(x[1]['effect_size']))
    return best_method[0]

def get_most_stable_method(methods_evaluation):
    """è·å–æœ€ç¨³å®šçš„æ–¹æ³•"""
    best_method = max(methods_evaluation.items(), key=lambda x: x[1]['stability_score'])
    return best_method[0]

def get_most_comprehensive_method(methods_evaluation):
    """è·å–æœ€ç»¼åˆçš„æ–¹æ³•ï¼ˆé€šå¸¸æ˜¯Hybridï¼‰"""
    if 'Hybrid RPSM' in methods_evaluation:
        return 'Hybrid RPSM'
    else:
        # è¿”å›æ•ˆåº”å¤§å°å’Œç¨³å®šæ€§éƒ½è¾ƒå¥½çš„æ–¹æ³•
        composite_scores = {}
        for method, eval_data in methods_evaluation.items():
            composite_scores[method] = (abs(eval_data['effect_size']) + eval_data['stability_score'] / 100) / 2
        return max(composite_scores.items(), key=lambda x: x[1])[0]

def create_rpsm_recommendation_visualization(methods_evaluation, recommendations, output_dir):
    """åˆ›å»ºRPSMæ¨èå¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    methods = list(methods_evaluation.keys())
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
    
    # 1. æ•ˆåº”å¤§å°å¯¹æ¯” (å·¦ä¸Š)
    ax = axes[0, 0]
    effect_sizes = [methods_evaluation[m]['effect_size'] for m in methods]
    bars = ax.bar(range(len(methods)), effect_sizes, color=colors, alpha=0.8)
    ax.set_title('Effect Size Comparison\n(Higher = Better Discrimination)')
    ax.set_ylabel('Cohen\'s d Effect Size')
    ax.set_xticks(range(len(methods)))
    ax.set_xticklabels([m.replace(' RPSM', '') for m in methods], rotation=45)
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Large Effect (d>0.5)')
    ax.axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='Medium Effect (d>0.3)')
    ax.legend()
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, effect in zip(bars, effect_sizes):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
               f'{effect:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 2. ä¸´åºŠå®ç”¨æ€§è¯„åˆ† (å³ä¸Š)
    ax = axes[0, 1]
    utility_scores = [methods_evaluation[m]['clinical_utility'] for m in methods]
    bars = ax.bar(range(len(methods)), utility_scores, color=colors, alpha=0.8)
    ax.set_title('Clinical Utility Score\n(0-100, Higher = More Practical)')
    ax.set_ylabel('Clinical Utility Score')
    ax.set_xticks(range(len(methods)))
    ax.set_xticklabels([m.replace(' RPSM', '') for m in methods], rotation=45)
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 100)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars, utility_scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 1,
               f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
    
    # 3. ç¨³å®šæ€§å¯¹æ¯” (å·¦ä¸‹)
    ax = axes[1, 0]
    stability_scores = [methods_evaluation[m]['stability_score'] for m in methods]
    bars = ax.bar(range(len(methods)), stability_scores, color=colors, alpha=0.8)
    ax.set_title('Method Stability Score\n(0-100, Higher = More Consistent)')
    ax.set_ylabel('Stability Score')
    ax.set_xticks(range(len(methods)))
    ax.set_xticklabels([m.replace(' RPSM', '') for m in methods], rotation=45)
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, 100)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars, stability_scores):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 1,
               f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
    
    # 4. æ¨èæ€»ç»“ (å³ä¸‹)
    ax = axes[1, 1]
    ax.axis('off')
    
    # åˆ›å»ºæ¨èæ–‡æœ¬
    rec_text = f"""
ğŸ† PRIMARY RECOMMENDATION
Method: {recommendations['primary_recommendation']['method']}
Score: {recommendations['primary_recommendation']['score']:.1f}/100
Rationale: {recommendations['primary_recommendation']['rationale']}

ğŸ¥ˆ ALTERNATIVE OPTIONS
"""
    for i, alt in enumerate(recommendations['alternative_recommendations'][:2]):
        rec_text += f"{i+2}. {alt['method']} (Score: {alt['score']:.1f})\n"
    
    rec_text += f"""
ğŸ¯ USE CASE RECOMMENDATIONS
â€¢ High Precision: {recommendations['use_case_specific']['high_precision_needed']}
â€¢ Clinical Screening: {recommendations['use_case_specific']['clinical_screening']}  
â€¢ Research: {recommendations['use_case_specific']['research_exploration']}

ğŸ“Š DATASET ASSESSMENT
â€¢ Sample Size: {recommendations['dataset_considerations']['sample_size']}
â€¢ Reliability: {recommendations['dataset_considerations']['recommendations_reliability']}
"""
    
    if recommendations['dataset_considerations']['suggested_validation']:
        rec_text += "âš ï¸ Validation recommended with larger dataset"
    
    ax.text(0.05, 0.95, rec_text, transform=ax.transAxes, fontsize=10,
           verticalalignment='top', fontfamily='monospace',
           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    
    plt.suptitle('RPSM Method Comprehensive Evaluation & Recommendations', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/rpsm_comprehensive_recommendations.png", dpi=300, bbox_inches='tight')
    plt.close()

def create_cell_distribution_plots(wsi_analyses, output_dir="plots"):
    """
    Create statistical plots for cell distribution - optimized for large WSI datasets
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Filter out None analyses
    valid_analyses = [a for a in wsi_analyses if a is not None]
    num_wsi = len(valid_analyses)
    
    if num_wsi == 0:
        print("No valid WSI analyses to plot")
        return
    
    # å½“WSIæ•°é‡è¿‡å¤šæ—¶ï¼Œåˆ‡æ¢åˆ°èšåˆåˆ†ææ¨¡å¼
    if num_wsi > 20:
        print(f"ğŸ“Š Large dataset detected ({num_wsi} WSIs), switching to aggregated analysis mode")
        create_aggregated_cell_distribution_plots(valid_analyses, output_dir)
        return
    
    cell_type_names = {
        1: "Neoplastic",
        2: "Inflammatory", 
        3: "Connective",
        4: "Dead",
        5: "Epithelial"
    }
    
    # ç”¨äºæ˜¾ç¤ºçš„ç®€çŸ­æ ‡ç­¾
    cell_type_short_names = {
        1: "Neo",      # Neoplastic cells
        2: "Inf",      # Inflammatory cells
        3: "Con",      # Connective tissue
        4: "Dead",     # Dead cells
        5: "Epi"       # Epithelial cells
    }
    
    cell_type_colors = {
        1: '#FF6B6B',  # Red - Neoplastic cells
        2: '#4ECDC4',  # Cyan - Inflammatory cells
        3: '#45B7D1',  # Blue - Connective tissue
        4: '#96CEB4',  # Green - Dead cells
        5: '#FECA57'   # Yellow - Epithelial cells
    }
    
    # Calculate optimal subplot layout  
    # å½“WSIæ•°é‡è¿‡å¤šæ—¶ï¼Œä½¿ç”¨æ›´ç´§å‡‘çš„å¸ƒå±€
    if num_wsi > 12:
        cols = 4  # ä½¿ç”¨4åˆ—å¸ƒå±€ä»¥èŠ‚çœç©ºé—´
        rows = (num_wsi + cols - 1) // cols
        fig_width = 4 * cols  # å‡å°æ¯ä¸ªå­å›¾çš„å®½åº¦
        fig_height = 3 * rows  # å‡å°æ¯ä¸ªå­å›¾çš„é«˜åº¦
    elif num_wsi > 6:
        cols = 3
        rows = (num_wsi + cols - 1) // cols
        fig_width = 5 * cols
        fig_height = 4 * rows
    else:
        cols = min(3, num_wsi)
        rows = (num_wsi + cols - 1) // cols
        fig_width = 5 * cols
        fig_height = 4 * rows
    
    # 1. Cell count distribution histogram
    fig, axes = plt.subplots(rows, cols, figsize=(fig_width, fig_height))
    
    # Ensure axes is always a flat array for consistent indexing
    if num_wsi == 1:
        axes = [axes]
    elif rows == 1 and cols == 1:
        axes = [axes]
    elif rows == 1 or cols == 1:
        axes = axes.flatten() if hasattr(axes, 'flatten') else axes
    else:
        axes = axes.flatten()
    
    for i, analysis in enumerate(valid_analyses):
        ax = axes[i]
        
        # Get total cell count for all patches
        total_cells_per_patch = [p['total_cells'] for p in analysis['patch_analyses']]
        
        ax.hist(total_cells_per_patch, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        ax.set_title(f"WSI {i+1}: Cell Count Distribution\n({'Responder' if analysis['label'] == 1 else 'Non-responder'})")
        ax.set_xlabel('Total cells per patch')
        ax.set_ylabel('Number of patches')
        ax.grid(True, alpha=0.3)
        
        # Add statistical information
        mean_cells = np.mean(total_cells_per_patch)
        median_cells = np.median(total_cells_per_patch)
        ax.axvline(mean_cells, color='red', linestyle='--', label=f'Mean: {mean_cells:.1f}')
        ax.axvline(median_cells, color='orange', linestyle='--', label=f'Median: {median_cells:.1f}')
        ax.legend(fontsize='small', loc='upper right')
    
    # Hide unused subplots
    for i in range(num_wsi, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/cell_count_distribution.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Cell type ratio boxplot
    fig, axes = plt.subplots(rows, cols, figsize=(fig_width, fig_height))
    
    # Ensure axes is always a flat array for consistent indexing
    if num_wsi == 1:
        axes = [axes]
    elif rows == 1 and cols == 1:
        axes = [axes]
    elif rows == 1 or cols == 1:
        axes = axes.flatten() if hasattr(axes, 'flatten') else axes
    else:
        axes = axes.flatten()
    
    for i, analysis in enumerate(valid_analyses):
        ax = axes[i]
        
        # Collect ratio data for each cell type
        cell_ratio_data = []
        cell_type_labels = []
        
        for cell_type in [1, 2, 3, 4, 5]:
            ratios = [p['cell_ratios'].get(str(cell_type), 0.0) for p in analysis['patch_analyses']]
            cell_ratio_data.append(ratios)
            # åœ¨æœ‰é™ç©ºé—´æ—¶ä½¿ç”¨çŸ­æ ‡ç­¾
            if num_wsi > 6:
                cell_type_labels.append(cell_type_short_names[cell_type])
            else:
                cell_type_labels.append(cell_type_names[cell_type])
        
        bp = ax.boxplot(cell_ratio_data, labels=cell_type_labels, patch_artist=True)
        
        # Set colors
        for patch, cell_type in zip(bp['boxes'], [1, 2, 3, 4, 5]):
            patch.set_facecolor(cell_type_colors[cell_type])
            patch.set_alpha(0.7)
        
        ax.set_title(f"WSI {i+1}: Cell Type Ratio Distribution\n({'Responder' if analysis['label'] == 1 else 'Non-responder'})")
        ax.set_ylabel('Cell ratio')
        ax.grid(True, alpha=0.3)
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
    
    # Hide unused subplots
    for i in range(num_wsi, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/cell_ratio_boxplot.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. RPSM selection analysis plots - COMMENTED OUT
    # This functionality is now replaced by more advanced analysis in:
    # - rpsm_comprehensive_evaluation.png 
    # - rpsm_roc_comparison_with_ci.png
    # - rpsm_selection_strictness.png
    
    """
    # å½“WSIæ•°é‡è¿‡å¤šæ—¶ï¼Œè°ƒæ•´å›¾è¡¨å¤§å°
    if num_wsi > 8:
        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    else:
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 3.1 RPSM selection rate comparison
    ax = axes[0, 0]
    wsi_names = []
    selection_rates = []
    response_labels = []
    
    for i, analysis in enumerate(valid_analyses):
        wsi_names.append(f"WSI {i+1}")
        selection_rates.append(analysis['rpsm_selected_count'] / analysis['total_patches'] * 100)
        response_labels.append('Responder' if analysis['label'] == 1 else 'Non-responder')
    
    # å½“WSIæ•°é‡è¿‡å¤šæ—¶ï¼Œä½¿ç”¨åˆ†ç»„æ˜¾ç¤ºç­–ç•¥
    if num_wsi > 15:
        # åˆ†ç»„æ˜¾ç¤ºï¼šæŒ‰å“åº”ç±»å‹åˆ†ç»„è®¡ç®—å¹³å‡å€¼
        responder_rates = [rate for rate, label in zip(selection_rates, response_labels) if label == 'Responder']
        non_responder_rates = [rate for rate, label in zip(selection_rates, response_labels) if label == 'Non-responder']
        
        group_names = ['Responder\nGroup', 'Non-responder\nGroup']
        group_means = [np.mean(responder_rates) if responder_rates else 0, 
                      np.mean(non_responder_rates) if non_responder_rates else 0]
        group_stds = [np.std(responder_rates) if len(responder_rates) > 1 else 0,
                     np.std(non_responder_rates) if len(non_responder_rates) > 1 else 0]
        
        bars = ax.bar(group_names, group_means, yerr=group_stds, 
                     color=['#FF6B6B', '#4ECDC4'], alpha=0.7, capsize=5)
        ax.set_title(f'RPSM Selection Rate Group Comparison\n(n={len(responder_rates)} responders, n={len(non_responder_rates)} non-responders)')
        ax.set_ylabel('Average selection rate (%)')
        
        # Add value labels
        for bar, mean, std in zip(bars, group_means, group_stds):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 1, 
                    f'{mean:.1f}Â±{std:.1f}%', ha='center', va='bottom', fontsize='small')
    else:
        # åŸæœ‰çš„ä¸ªä½“æ˜¾ç¤ºç­–ç•¥
        colors = ['#FF6B6B' if label == 'Responder' else '#4ECDC4' for label in response_labels]
        bars = ax.bar(wsi_names, selection_rates, color=colors, alpha=0.7)
        ax.set_title('RPSM Selection Rate Comparison')
        ax.set_ylabel('Selection rate (%)')
        
        # å½“WSIæ•°é‡è¿‡å¤šæ—¶ï¼Œæ—‹è½¬xè½´æ ‡ç­¾
        if num_wsi > 6:
            plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize='small')
        
        # Add value labels
        for bar, rate in zip(bars, selection_rates):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{rate:.1f}%', ha='center', va='bottom', fontsize='small')
        
        # æ·»åŠ å›¾ä¾‹ï¼ŒåŒºåˆ†å“åº”è€…å’Œéå“åº”è€…
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='#FF6B6B', alpha=0.7, label='Responder'),
                          Patch(facecolor='#4ECDC4', alpha=0.7, label='Non-responder')]
        ax.legend(handles=legend_elements, fontsize='small', loc='upper right')
    
    ax.grid(True, alpha=0.3)
    
    # 3.2 RPSM selection reason distribution
    ax = axes[0, 1]
    all_reasons = []
    for analysis in valid_analyses:
        selected_patches = [p for p in analysis['patch_analyses'] if p['rpsm_selected']]
        reasons = [p['rpsm_reason'] for p in selected_patches]
        all_reasons.extend(reasons)
    
    if all_reasons:
        reason_counts = pd.Series(all_reasons).value_counts()
        colors_pie = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        ax.pie(reason_counts.values, labels=reason_counts.index, autopct='%1.1f%%', 
               colors=colors_pie[:len(reason_counts)])
        ax.set_title('RPSM Selection Reason Distribution')
    
    # 3.3 Total cell count vs RPSM selection
    ax = axes[1, 0]
    for i, analysis in enumerate(valid_analyses):
        selected_cells = [p['total_cells'] for p in analysis['patch_analyses'] if p['rpsm_selected']]
        unselected_cells = [p['total_cells'] for p in analysis['patch_analyses'] if not p['rpsm_selected']]
        
        ax.hist(selected_cells, bins=20, alpha=0.7, label=f'WSI {i+1} Selected', color=colors[i])
        ax.hist(unselected_cells, bins=20, alpha=0.3, label=f'WSI {i+1} Unselected', color=colors[i])
    
    ax.set_title('RPSM Selection vs Total Cell Count')
    ax.set_xlabel('Total cell count')
    ax.set_ylabel('Number of patches')
    
    # ä¼˜åŒ–å›¾ä¾‹æ˜¾ç¤ºï¼Œé˜²æ­¢é®æŒ¡å›¾è¡¨
    if len(valid_analyses) > 4:
        # å½“WSIæ•°é‡è¿‡å¤šæ—¶ï¼Œå°†å›¾ä¾‹æ”¾ç½®åœ¨å›¾è¡¨å¤–éƒ¨
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
    else:
        # WSIæ•°é‡è¾ƒå°‘æ—¶ï¼Œä½¿ç”¨é»˜è®¤ä½ç½®
        ax.legend(fontsize='small')
    ax.grid(True, alpha=0.3)
    
    # 3.4 Cell type preference in RPSM selection
    ax = axes[1, 1]
    selected_ratios = {str(cell_type): [] for cell_type in [1, 2, 3, 4, 5]}
    
    for analysis in valid_analyses:
        selected_patches = [p for p in analysis['patch_analyses'] if p['rpsm_selected']]
        for patch in selected_patches:
            for cell_type in [1, 2, 3, 4, 5]:
                cell_type_str = str(cell_type)
                selected_ratios[cell_type_str].append(patch['cell_ratios'].get(cell_type_str, 0.0))
    
    if any(selected_ratios.values()):
        mean_ratios = [np.mean(selected_ratios[str(cell_type)]) if selected_ratios[str(cell_type)] else 0 
                      for cell_type in [1, 2, 3, 4, 5]]
        
        bars = ax.bar(range(5), mean_ratios, 
                     color=[cell_type_colors[i+1] for i in range(5)], alpha=0.7)
        ax.set_title('Average Cell Ratio in RPSM Selected Patches')
        ax.set_xlabel('Cell type')
        ax.set_ylabel('Average ratio')
        ax.set_xticks(range(5))
        # å½“æ ·æœ¬è¾ƒå¤šæ—¶ä½¿ç”¨çŸ­æ ‡ç­¾
        if num_wsi > 6:
            ax.set_xticklabels([cell_type_short_names[i+1] for i in range(5)], rotation=45, ha='right')
        else:
            ax.set_xticklabels([cell_type_names[i+1] for i in range(5)], rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        # Add value labels
        for bar, ratio in zip(bars, mean_ratios):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{ratio:.2f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/rpsm_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()
    """
    
    # 4. Cell type correlation heatmap
    fig, axes = plt.subplots(rows, cols, figsize=(fig_width, fig_height))
    
    # Ensure axes is always a flat array for consistent indexing
    if num_wsi == 1:
        axes = [axes]
    elif rows == 1 and cols == 1:
        axes = [axes]
    elif rows == 1 or cols == 1:
        axes = axes.flatten() if hasattr(axes, 'flatten') else axes
    else:
        axes = axes.flatten()
    
    for i, analysis in enumerate(valid_analyses):
        ax = axes[i]
        
        # Build cell ratio data matrix
        ratio_data = []
        # æ ¹æ®WSIæ•°é‡å†³å®šä½¿ç”¨é•¿æ ‡ç­¾è¿˜æ˜¯çŸ­æ ‡ç­¾
        if num_wsi > 6:
            column_names = [cell_type_short_names[j] for j in [1, 2, 3, 4, 5]]
        else:
            column_names = [cell_type_names[j] for j in [1, 2, 3, 4, 5]]
        
        for patch in analysis['patch_analyses']:
            ratio_data.append([patch['cell_ratios'].get(str(j), 0.0) for j in [1, 2, 3, 4, 5]])
        
        ratio_df = pd.DataFrame(data=ratio_data, columns=column_names)
        
        # Calculate correlation matrix
        corr_matrix = ratio_df.corr()
        
        # Plot heatmap
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                   square=True, ax=ax, cbar_kws={'shrink': 0.8})
        ax.set_title(f"WSI {i+1}: Cell Type Correlation\n({'Responder' if analysis['label'] == 1 else 'Non-responder'})")
    
    # Hide unused subplots
    for i in range(num_wsi, len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/cell_correlation_heatmap.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Statistical plots saved to {output_dir} directory")

def create_patch_examples_visualization(wsi_analyses, output_dir="plots"):
    """
    åˆ›å»ºæ¯ä¸ªWSIçš„patchå®ä¾‹å¯è§†åŒ–ï¼Œå±•ç¤ºRPSMé€‰æ‹©çš„å…·ä½“ä¾‹å­
    """
    print("ğŸ¨ Starting patch examples visualization...")
    os.makedirs(output_dir, exist_ok=True)
    
    valid_analyses = [a for a in wsi_analyses if a is not None]
    if not valid_analyses:
        print("âŒ No valid analyses found for patch examples visualization")
        return
    
    print(f"ğŸ“Š Found {len(valid_analyses)} valid WSI analyses")
    
    cell_type_names = {
        1: "Neoplastic", 2: "Inflammatory", 3: "Connective", 4: "Dead", 5: "Epithelial"
    }
    
    cell_type_colors = {
        1: '#FF6B6B', 2: '#4ECDC4', 3: '#45B7D1', 4: '#96CEB4', 5: '#FECA57'
    }
    
    for wsi_idx, analysis in enumerate(valid_analyses):
        print(f"ğŸ” Processing WSI {wsi_idx + 1}/{len(valid_analyses)}...")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥analysisç»“æ„
        if 'patch_analyses' not in analysis:
            print(f"âŒ No patch_analyses found in WSI {wsi_idx + 1}")
            continue
            
        total_patches = len(analysis['patch_analyses'])
        print(f"ğŸ“‹ WSI {wsi_idx + 1} has {total_patches} patches")
        
        # é€‰æ‹©æœ‰ä»£è¡¨æ€§çš„patchï¼šRPSMé€‰ä¸­çš„ã€æœªé€‰ä¸­çš„ã€ä¸‰ç§RPSMæ ‡å‡†çš„å¯¹æ¯”
        selected_patches = [p for p in analysis['patch_analyses'] if p.get('rpsm_selected', False)]
        unselected_patches = [p for p in analysis['patch_analyses'] if not p.get('rpsm_selected', False)]
        improved_selected = [p for p in analysis['patch_analyses'] if p.get('improved_rpsm_selected', False)]
        angio_selected = [p for p in analysis['patch_analyses'] if p.get('angio_rpsm_selected', False)]
        
        print(f"   - Original RPSM selected: {len(selected_patches)}")
        print(f"   - Improved RPSM selected: {len(improved_selected)}")
        print(f"   - Angiogenesis RPSM selected: {len(angio_selected)}")
        print(f"   - Unselected patches: {len(unselected_patches)}")
        
        # é€‰æ‹©æœ€å¤š6ä¸ªæœ‰ä»£è¡¨æ€§çš„patchè¿›è¡Œå±•ç¤º
        example_patches = []
        
        # æ·»åŠ åŸå§‹RPSMé€‰ä¸­çš„patchï¼ˆæœ€å¤š2ä¸ªï¼‰
        if selected_patches:
            example_patches.extend(selected_patches[:2])
            print(f"   - Added {min(2, len(selected_patches))} original RPSM patches")
        
        # æ·»åŠ æ”¹è¿›RPSMç‰¹æœ‰é€‰ä¸­çš„patchï¼ˆæœ€å¤š2ä¸ªï¼‰
        improved_only = [p for p in improved_selected if not p.get('rpsm_selected', False)]
        if improved_only:
            example_patches.extend(improved_only[:2])
            print(f"   - Added {min(2, len(improved_only))} improved RPSM only patches")
        
        # æ·»åŠ è¡€ç®¡ç”ŸæˆRPSMç‰¹æœ‰é€‰ä¸­çš„patchï¼ˆæœ€å¤š2ä¸ªï¼‰
        angio_only = [p for p in angio_selected if not p.get('rpsm_selected', False) and not p.get('improved_rpsm_selected', False)]
        if angio_only:
            example_patches.extend(angio_only[:2])
            print(f"   - Added {min(2, len(angio_only))} angiogenesis RPSM only patches")
        
        # å¦‚æœä¾‹å­ä¸å¤Ÿï¼Œæ·»åŠ ä¸€äº›æœªé€‰ä¸­çš„patchä½œä¸ºå¯¹æ¯”
        if len(example_patches) < 4 and unselected_patches:
            needed = 4 - len(example_patches)
            example_patches.extend(unselected_patches[:needed])
            print(f"   - Added {min(needed, len(unselected_patches))} unselected patches for comparison")
        
        if not example_patches:
            print(f"âŒ No example patches found for WSI {wsi_idx + 1}")
            continue
        
        # é™åˆ¶æœ€å¤šå±•ç¤º6ä¸ªpatch
        example_patches = example_patches[:6]
        print(f"ğŸ“ Will visualize {len(example_patches)} patches for WSI {wsi_idx + 1}")
        
        # åˆ›å»ºå­å›¾
        n_patches = len(example_patches)
        cols = min(3, n_patches)
        rows = (n_patches + cols - 1) // cols
        
        print(f"ğŸ¯ Creating {rows}x{cols} subplot layout")
        
        fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))
        if n_patches == 1:
            axes = [axes]
        elif rows == 1:
            axes = axes if isinstance(axes, (list, np.ndarray)) else [axes]
        else:
            axes = axes.flatten()
        
        for patch_idx, patch in enumerate(example_patches):
            ax = axes[patch_idx]
            
            print(f"   ğŸ“Š Processing patch {patch_idx + 1}/{len(example_patches)}")
            
            # è°ƒè¯•ï¼šæ£€æŸ¥patchæ•°æ®ç»“æ„
            if 'cell_counts' not in patch:
                print(f"     âŒ No cell_counts in patch {patch_idx + 1}")
                ax.text(0.5, 0.5, 'No cell data', ha='center', va='center', 
                       transform=ax.transAxes, fontsize=12)
                continue
                
            # åˆ›å»ºç»†èƒç±»å‹åˆ†å¸ƒé¥¼å›¾
            cell_counts = patch['cell_counts']
            print(f"     ğŸ“‹ Cell counts: {cell_counts}")
            
            # ä¿®å¤ï¼šcell_countsä½¿ç”¨å­—ç¬¦ä¸²é”®ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²è®¿é—®
            sizes = [cell_counts.get(str(i), 0) for i in range(1, 6)]
            print(f"     ğŸ“Š Sizes: {sizes}")
            
            labels = [cell_type_names[i] for i in range(1, 6)]
            colors = [cell_type_colors[i] for i in range(1, 6)]
            
            # åªæ˜¾ç¤ºéé›¶çš„ç»†èƒç±»å‹
            non_zero_indices = [i for i, size in enumerate(sizes) if size > 0]
            print(f"     âœ… Non-zero indices: {non_zero_indices}")
            
            if non_zero_indices:
                filtered_sizes = [sizes[i] for i in non_zero_indices]
                filtered_labels = [labels[i] for i in non_zero_indices]
                filtered_colors = [colors[i] for i in non_zero_indices]
                
                print(f"     ğŸ¨ Creating pie chart with {len(filtered_sizes)} segments")
                
                wedges, texts, autotexts = ax.pie(filtered_sizes, labels=filtered_labels, 
                                                colors=filtered_colors, autopct='%1.1f%%', 
                                                startangle=90)
                
                # è®¾ç½®æ–‡å­—å¤§å°
                for text in texts:
                    text.set_fontsize(8)
                for autotext in autotexts:
                    autotext.set_fontsize(7)
                    autotext.set_color('white')
                    autotext.set_weight('bold')
            else:
                # å¦‚æœæ²¡æœ‰ç»†èƒï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
                print(f"     âš ï¸ No cells detected in patch {patch_idx + 1}")
                ax.text(0.5, 0.5, 'No cells detected', ha='center', va='center', 
                       transform=ax.transAxes, fontsize=12)
                ax.set_xlim(0, 1)
                ax.set_ylim(0, 1)
            
            # æ ‡é¢˜ä¿¡æ¯
            rpsm_status = []
            if patch.get('rpsm_selected', False):
                rpsm_status.append(f"Original: {patch.get('rpsm_reason', 'Selected')}")
            if patch.get('improved_rpsm_selected', False):
                rpsm_status.append(f"Improved: {patch.get('improved_rpsm_reason', 'Selected')}")
            if patch.get('angio_rpsm_selected', False):
                rpsm_status.append(f"Angio: {patch.get('angio_rpsm_reason', 'Selected')}")
            
            if not rpsm_status:
                rpsm_status = ["Not selected by any RPSM"]
            
            total_cells = patch.get('total_cells', 0)
            title = f"Patch {patch_idx+1}\nTotal cells: {total_cells}\n" + "\n".join(rpsm_status)
            ax.set_title(title, fontsize=9, pad=10)
            
            print(f"     âœ… Patch {patch_idx + 1} visualization completed")
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_patches, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        output_path = f"{output_dir}/wsi_{wsi_idx+1}_patch_examples.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… WSI {wsi_idx + 1} patch examples saved to {output_path}")
    
    print(f"ğŸ‰ Patch examples visualizations completed and saved to {output_dir} directory")

def create_rpsm_comparison_visualization(wsi_analyses, output_dir="plots"):
    """
    åˆ›å»ºRPSMæ–¹æ³•è¯¦ç»†å¯¹æ¯”åˆ†æ - ä¸“æ³¨äºæ ¸å¿ƒæ–¹æ³•å·®å¼‚å’Œç”Ÿç‰©å­¦æ´å¯Ÿ
    
    ä¼˜åŒ–å†…å®¹:
    1. æ–¹æ³•ä¸¥æ ¼åº¦åˆ†å¸ƒåˆ†æ  
    2. Patchè´¨é‡è¯„åˆ†å¯¹æ¯”
    3. ç»†èƒç±»å‹åå¥½çƒ­åŠ›å›¾
    4. å“åº”è€…vséå“åº”è€…ç»†èƒåˆ†å¸ƒ
    
    ç§»é™¤å†—ä½™å†…å®¹:
    - æ–¹æ³•é€‰æ‹©é‡å åˆ†æ (å·²è¢«performance comparisonæ›¿ä»£)
    - å¤±è´¥æ¡ˆä¾‹åˆ†æ (å·²è¢«detailed analysisä¸­å¼‚å¸¸æ£€æµ‹æ›¿ä»£)
    """
    os.makedirs(output_dir, exist_ok=True)
    
    valid_analyses = [a for a in wsi_analyses if a is not None]
    if not valid_analyses:
        return
    
    # RPSMæ–¹æ³•è¯¦ç»†å¯¹æ¯”åˆ†æ - ä¼˜åŒ–ä¸º2x2å¸ƒå±€ï¼Œçªå‡ºæ ¸å¿ƒä¿¡æ¯
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    responder_analyses = [a for a in valid_analyses if a['label'] == 1]
    non_responder_analyses = [a for a in valid_analyses if a['label'] == 0]
    
    # 1.1 RPSMæ–¹æ³•ä¸¥æ ¼åº¦åˆ†æ
    ax = axes[0, 0]
    methods = ['Original', 'Improved', 'Angiogenesis', 'Hybrid']
    
    # è®¡ç®—æ¯ç§æ–¹æ³•çš„å¹³å‡é€‰æ‹©ç‡ï¼ˆä¸¥æ ¼åº¦çš„åæ˜ ï¼‰
    strictness_data = []
    for method_key in ['rpsm_selected_count', 'improved_rpsm_selected_count', 'angio_rpsm_selected_count', 'hybrid_rpsm_selected_count']:
        rates = []
        for analysis in valid_analyses:
            if method_key in analysis:
                rate = analysis[method_key] / analysis['total_patches'] * 100
                rates.append(rate)
        strictness_data.append(rates)
    
    # åˆ›å»ºå°æç´å›¾æ˜¾ç¤ºåˆ†å¸ƒ
    if strictness_data and any(strictness_data):
        violin_parts = ax.violinplot(strictness_data, positions=range(len(methods)), showmeans=True, showmedians=True)
        ax.set_xticks(range(len(methods)))
        ax.set_xticklabels(methods, rotation=45)
        ax.set_ylabel('Selection Rate (%)')
        ax.set_title('RPSM Method Strictness Distribution')
        ax.grid(True, alpha=0.3)
        
        # è®¾ç½®é¢œè‰²
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        for pc, color in zip(violin_parts['bodies'], colors):
            pc.set_facecolor(color)
            pc.set_alpha(0.7)
    
    # 1.2 Patchè´¨é‡è¯„åˆ†åˆ†å¸ƒå¯¹æ¯”
    ax = axes[0, 1]
    
    def calculate_simple_quality_score(cell_counts):
        total_cells = sum(cell_counts.values())
        if total_cells == 0:
            return 0
        
        # ç»†èƒå¯†åº¦è¯„åˆ†
        density_score = min(total_cells / 200.0, 1.0) * 0.4
        
        # ç»†èƒå¤šæ ·æ€§è¯„åˆ†
        cell_types = sum(1 for count in cell_counts.values() if count > 0)
        diversity_score = (cell_types / 5.0) * 0.6
        
        return density_score + diversity_score
    
    quality_data = []
    quality_labels = []
    
    for method_name, patches_key in [
        ('Original', 'rpsm_selected_patches'),
        ('Improved', 'improved_rpsm_selected_patches'),
        ('Angiogenesis', 'angio_rpsm_selected_patches'),
        ('Hybrid', 'hybrid_rpsm_selected_patches')
    ]:
        method_quality = []
        for analysis in valid_analyses:
            if patches_key in analysis and analysis[patches_key]:
                for patch in analysis[patches_key]:
                    if 'cell_counts' in patch:
                        quality = calculate_simple_quality_score(patch['cell_counts'])
                        method_quality.append(quality)
        
        if method_quality:
            quality_data.append(method_quality)
            quality_labels.append(method_name)
    
    if quality_data:
        bp = ax.boxplot(quality_data, labels=quality_labels, patch_artist=True)
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        ax.set_ylabel('Quality Score')
        ax.set_title('Patch Quality Score Distribution by Method')
        plt.setp(ax.get_xticklabels(), rotation=45)
        ax.grid(True, alpha=0.3)
    
    # 1.3 ç»†èƒç±»å‹åå¥½çƒ­åŠ›å›¾ (å³ä¸Š)
    ax = axes[0, 1]
    
    # è®¡ç®—æ¯ç§æ–¹æ³•å¯¹ä¸åŒç»†èƒç±»å‹çš„åå¥½
    cell_preferences_matrix = []
    method_names = ['Original', 'Improved', 'Angiogenesis', 'Hybrid']
    cell_type_names = ['Neoplastic', 'Inflammatory', 'Connective', 'Dead', 'Epithelial']
    
    for method_name, patches_key in [
        ('Original', 'rpsm_selected_patches'),
        ('Improved', 'improved_rpsm_selected_patches'),
        ('Angiogenesis', 'angio_rpsm_selected_patches'),
        ('Hybrid', 'hybrid_rpsm_selected_patches')
    ]:
        method_cell_ratios = []
        for analysis in valid_analyses:
            if patches_key in analysis and analysis[patches_key]:
                for patch in analysis[patches_key]:
                    if 'cell_ratios' in patch:
                        ratios = [patch['cell_ratios'].get(str(i), 0.0) for i in range(1, 6)]
                        method_cell_ratios.append(ratios)
        
        if method_cell_ratios:
            avg_ratios = np.mean(method_cell_ratios, axis=0)
            cell_preferences_matrix.append(avg_ratios)
        else:
            cell_preferences_matrix.append([0, 0, 0, 0, 0])
    
    if cell_preferences_matrix:
        cell_preferences_matrix = np.array(cell_preferences_matrix)
        im = ax.imshow(cell_preferences_matrix, cmap='RdYlBu_r', aspect='auto')
        
        ax.set_xticks(range(5))
        ax.set_xticklabels(['Neo', 'Inf', 'Con', 'Dead', 'Epi'], rotation=45)
        ax.set_yticks(range(len(method_names)))
        ax.set_yticklabels(method_names)
        ax.set_title('Cell Type Preference Heatmap')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(method_names)):
            for j in range(5):
                if len(cell_preferences_matrix) > i:
                    text = ax.text(j, i, f'{cell_preferences_matrix[i, j]:.2f}',
                                 ha="center", va="center", color="black", fontsize=9)
        
        plt.colorbar(im, ax=ax, label='Average Cell Ratio')
    
    # 1.4 å“åº”è€…vséå“åº”è€…çš„ç»†èƒåˆ†å¸ƒå¯¹æ¯” (å·¦ä¸‹)
    ax = axes[1, 0]
    
    if responder_analyses and non_responder_analyses:
        # è®¡ç®—æ¯ç»„çš„å¹³å‡ç»†èƒæ¯”ä¾‹
        resp_cell_ratios = {str(i): [] for i in range(1, 6)}
        non_resp_cell_ratios = {str(i): [] for i in range(1, 6)}
        
        for analysis in responder_analyses:
            for patch in analysis['patch_analyses']:
                for cell_type in range(1, 6):
                    cell_type_str = str(cell_type)
                    resp_cell_ratios[cell_type_str].append(patch['cell_ratios'].get(cell_type_str, 0.0))
        
        for analysis in non_responder_analyses:
            for patch in analysis['patch_analyses']:
                for cell_type in range(1, 6):
                    cell_type_str = str(cell_type)
                    non_resp_cell_ratios[cell_type_str].append(patch['cell_ratios'].get(cell_type_str, 0.0))
        
        resp_means = [np.mean(resp_cell_ratios[str(i)]) for i in range(1, 6)]
        non_resp_means = [np.mean(non_resp_cell_ratios[str(i)]) for i in range(1, 6)]
        
        x = np.arange(5)
        width = 0.35
        
        bars1 = ax.bar(x - width/2, resp_means, width, label='Responders', color='#2E8B57', alpha=0.8)
        bars2 = ax.bar(x + width/2, non_resp_means, width, label='Non-responders', color='#CD5C5C', alpha=0.8)
        
        ax.set_title('Cell Distribution: Responders vs Non-responders')
        ax.set_ylabel('Average Cell Ratio')
        ax.set_xlabel('Cell Types')
        ax.set_xticks(x)
        ax.set_xticklabels(['Neo', 'Inf', 'Con', 'Dead', 'Epi'])
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.2f}', ha='center', va='bottom', fontsize=9)
        for bar in bars2:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.2f}', ha='center', va='bottom', fontsize=9)
    
    # 1.4 æ–¹æ³•é€‰æ‹©æ•ˆç‡å¯¹æ¯” (å³ä¸‹)
    ax = axes[1, 1]
    
    # è®¡ç®—æ¯ç§æ–¹æ³•çš„é€‰æ‹©æ•ˆç‡æŒ‡æ ‡
    efficiency_metrics = {'Original': [], 'Improved': [], 'Angiogenesis': [], 'Hybrid': []}
    
    for analysis in valid_analyses:
        total_patches = analysis['total_patches']
        
        # è®¡ç®—å„æ–¹æ³•çš„é€‰æ‹©æ•ˆç‡ (é€‰æ‹©ç‡ Ã— è´¨é‡åˆ†æ•°)
        methods_data = [
            ('Original', analysis.get('rpsm_selected_count', 0), 'rpsm_selected'),
            ('Improved', analysis.get('improved_rpsm_selected_count', 0), 'improved_rpsm_selected'),
            ('Angiogenesis', analysis.get('angio_rpsm_selected_count', 0), 'angio_rpsm_selected'),
            ('Hybrid', analysis.get('hybrid_rpsm_selected_count', 0), 'hybrid_rpsm_selected')
        ]
        
        for method_name, selected_count, patch_key in methods_data:
            if selected_count > 0:
                # è®¡ç®—é€‰ä¸­patchesçš„å¹³å‡è´¨é‡
                selected_patches = [p for p in analysis['patch_analyses'] if p.get(patch_key, False)]
                if selected_patches:
                    avg_quality = np.mean([calculate_simple_quality_score(p['cell_counts']) for p in selected_patches])
                    selection_rate = selected_count / total_patches
                    efficiency = selection_rate * avg_quality * 100  # æ•ˆç‡æŒ‡æ ‡
                    efficiency_metrics[method_name].append(efficiency)
                else:
                    efficiency_metrics[method_name].append(0)
            else:
                efficiency_metrics[method_name].append(0)
    
    # ç»˜åˆ¶æ•ˆç‡å¯¹æ¯”
    method_names = list(efficiency_metrics.keys())
    efficiency_data = [efficiency_metrics[method] for method in method_names]
    
    if efficiency_data and any(any(data) for data in efficiency_data):
        bp = ax.boxplot(efficiency_data, labels=method_names, patch_artist=True)
        ax.set_title('RPSM Method Selection Efficiency')
        ax.set_ylabel('Efficiency Score (Selection Rate Ã— Quality)')
        ax.grid(True, alpha=0.3)
        plt.setp(ax.get_xticklabels(), rotation=45)
        
        # è®¾ç½®é¢œè‰²
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
    
    plt.suptitle('RPSM Methods Core Comparison Analysis', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/rpsm_detailed_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()

def calculate_patch_quality(patch):
    """è®¡ç®—patchçš„è´¨é‡åˆ†æ•°"""
    try:
        total_cells = patch['total_cells']
        if total_cells == 0:
            return 0.0
        
        # ç»†èƒç±»å‹å¤šæ ·æ€§
        cell_counts = patch['cell_counts']
        non_zero_types = sum(1 for count in cell_counts.values() if count > 0)
        diversity_score = min(non_zero_types / 5.0, 1.0)
        
        # ç»†èƒå¯†åº¦é€‚ä¸­æ€§ (50-300 cells optimal)
        if 50 <= total_cells <= 300:
            density_score = 1.0
        elif total_cells < 50:
            density_score = total_cells / 50.0
        else:
            density_score = max(0.3, 300.0 / total_cells)
        
        # è‚¿ç˜¤ç»†èƒæ¯”ä¾‹é€‚ä¸­
        tumor_ratio = patch['cell_ratios'].get('1', 0.0)
        if 0.3 <= tumor_ratio <= 0.8:
            tumor_score = 1.0
        elif tumor_ratio < 0.3:
            tumor_score = tumor_ratio / 0.3
        else:
            tumor_score = max(0.5, (1.0 - tumor_ratio) / 0.2)
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = (diversity_score * 0.4 + density_score * 0.3 + tumor_score * 0.3)
        return min(quality_score, 1.0)
    except:
        return 0.0

def calculate_angiogenesis_score(patch):
    """è®¡ç®—patchçš„è¡€ç®¡ç”Ÿæˆåˆ†æ•°"""
    try:
        cell_ratios = patch['cell_ratios']
        
        # è¡€ç®¡ç”Ÿæˆç›¸å…³çš„ç»†èƒæ¯”ä¾‹ç»„åˆ
        endothelial_ratio = cell_ratios.get('2', 0.0)  # å†…çš®ç»†èƒ
        inflammatory_ratio = cell_ratios.get('3', 0.0)  # ç‚ç—‡ç»†èƒ
        connective_ratio = cell_ratios.get('4', 0.0)   # ç»“ç¼”ç»„ç»‡ç»†èƒ
        
        # è¡€ç®¡ç”Ÿæˆè¯„åˆ†ç®—æ³•
        angio_score = (
            endothelial_ratio * 0.5 +  # å†…çš®ç»†èƒæœ€é‡è¦
            inflammatory_ratio * 0.3 +  # ç‚ç—‡ç»†èƒæ¬¡è¦
            connective_ratio * 0.2      # ç»“ç¼”ç»„ç»‡æ”¯æŒ
        )
        
        return min(angio_score, 1.0)
    except:
        return 0.0

def calculate_immune_infiltration(patch):
    """è®¡ç®—patchçš„å…ç–«æµ¸æ¶¦åˆ†æ•°"""
    try:
        cell_ratios = patch['cell_ratios']
        
        # å…ç–«ç›¸å…³ç»†èƒæ¯”ä¾‹
        inflammatory_ratio = cell_ratios.get('3', 0.0)  # ç‚ç—‡/å…ç–«ç»†èƒ
        necrotic_ratio = cell_ratios.get('5', 0.0)      # åæ­»ç»†èƒ(å¯èƒ½åŒ…å«å…ç–«ç»†èƒ)
        
        # å…ç–«æµ¸æ¶¦è¯„åˆ†
        immune_score = inflammatory_ratio * 0.8 + necrotic_ratio * 0.2
        
        return min(immune_score, 1.0)
    except:
        return 0.0

def calculate_clinical_scores(patches):
    """è®¡ç®—ä¸´åºŠç›¸å…³æ€§è¯„åˆ†"""
    try:
        if not patches:
            return {'angiogenesis': 0.0, 'immune': 0.0, 'tumor_burden': 0.0, 'quality': 0.0}
        
        # è®¡ç®—æ‰€æœ‰patchçš„å„é¡¹åˆ†æ•°
        angio_scores = [calculate_angiogenesis_score(p) for p in patches]
        immune_scores = [calculate_immune_infiltration(p) for p in patches]
        quality_scores = [calculate_patch_quality(p) for p in patches]
        
        # è‚¿ç˜¤è´Ÿè·åˆ†æ•°
        tumor_burdens = []
        for patch in patches:
            tumor_ratio = patch['cell_ratios'].get('1', 0.0)
            total_cells = patch['total_cells']
            # ç»“åˆè‚¿ç˜¤æ¯”ä¾‹å’Œç»†èƒå¯†åº¦
            burden = tumor_ratio * min(total_cells / 200.0, 1.0)
            tumor_burdens.append(burden)
        
        return {
            'angiogenesis': np.mean(angio_scores),
            'immune': np.mean(immune_scores),
            'tumor_burden': np.mean(tumor_burdens),
            'quality': np.mean(quality_scores)
        }
    except:
        return {'angiogenesis': 0.0, 'immune': 0.0, 'tumor_burden': 0.0, 'quality': 0.0}
    
def create_detailed_patch_analysis(wsi_analyses, output_dir="plots"):
    """
    åˆ›å»ºä¼˜åŒ–çš„WSIè¯¦ç»†åˆ†æå›¾è¡¨ - æ™ºèƒ½é‡‡æ ·æ¨¡å¼é€‚åº”å¤§è§„æ¨¡æ•°æ®é›†
    
    ä¼˜åŒ–ç­–ç•¥:
    - WSI <= 10: å…¨éƒ¨è¯¦ç»†åˆ†æ
    - WSI 11-50: é‡‡æ ·åˆ†æ + èšåˆç»Ÿè®¡
    - WSI > 50: ä»…ç”Ÿæˆä»£è¡¨æ€§æ ·æœ¬åˆ†æ
    """
    os.makedirs(output_dir, exist_ok=True)
    
    valid_analyses = [a for a in wsi_analyses if a is not None]
    if not valid_analyses:
        return
    
    num_wsi = len(valid_analyses)
    
    # æ™ºèƒ½é‡‡æ ·ç­–ç•¥
    if num_wsi <= 10:
        # å°è§„æ¨¡æ•°æ®é›†ï¼šå…¨éƒ¨åˆ†æ
        selected_analyses = valid_analyses
        print(f"ğŸ“Š Small dataset: analyzing all {num_wsi} WSIs")
    elif num_wsi <= 50:
        # ä¸­ç­‰è§„æ¨¡ï¼šé‡‡æ ·åˆ†æ
        # ç¡®ä¿å“åº”è€…å’Œéå“åº”è€…çš„ä»£è¡¨æ€§
        responders = [a for a in valid_analyses if a['label'] == 1]
        non_responders = [a for a in valid_analyses if a['label'] == 0]
        
        # ä»æ¯ç»„é‡‡æ ·æœ€å¤š5ä¸ª
        sampled_responders = random.sample(responders, min(5, len(responders))) if responders else []
        sampled_non_responders = random.sample(non_responders, min(5, len(non_responders))) if non_responders else []
        
        selected_analyses = sampled_responders + sampled_non_responders
        print(f"ğŸ“Š Medium dataset: sampling {len(selected_analyses)} from {num_wsi} WSIs ({len(sampled_responders)} responders, {len(sampled_non_responders)} non-responders)")
    else:
        # å¤§è§„æ¨¡æ•°æ®é›†ï¼šä»…ç”Ÿæˆä»£è¡¨æ€§æ ·æœ¬
        responders = [a for a in valid_analyses if a['label'] == 1]
        non_responders = [a for a in valid_analyses if a['label'] == 0]
        
        # ä»æ¯ç»„é‡‡æ ·æœ€å¤š3ä¸ªæœ€æœ‰ä»£è¡¨æ€§çš„æ¡ˆä¾‹
        # åŸºäºRPSMé€‰æ‹©ç‡é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„æ¡ˆä¾‹
        if responders:
            responders_sorted = sorted(responders, key=lambda x: x['rpsm_selected_count'] / x['total_patches'], reverse=True)
            sampled_responders = responders_sorted[:3]
        else:
            sampled_responders = []
            
        if non_responders:
            non_responders_sorted = sorted(non_responders, key=lambda x: x['rpsm_selected_count'] / x['total_patches'], reverse=True)
            sampled_non_responders = non_responders_sorted[:3]
        else:
            sampled_non_responders = []
        
        selected_analyses = sampled_responders + sampled_non_responders
        print(f"ğŸ“Š Large dataset: analyzing {len(selected_analyses)} representative cases from {num_wsi} WSIs")
    
    # ä¸ºå¤§è§„æ¨¡æ•°æ®é›†ç”Ÿæˆé‡‡æ ·è¯´æ˜æ–‡ä»¶
    if num_wsi > 10:
        sampling_info = {
            'total_wsi': num_wsi,
            'analyzed_wsi': len(selected_analyses),
            'sampling_strategy': 'representative' if num_wsi > 50 else 'random',
            'responders_total': len([a for a in valid_analyses if a['label'] == 1]),
            'responders_sampled': len([a for a in selected_analyses if a['label'] == 1]),
            'non_responders_total': len([a for a in valid_analyses if a['label'] == 0]),
            'non_responders_sampled': len([a for a in selected_analyses if a['label'] == 0])
        }
        
        with open(f"{output_dir}/sampling_info.json", 'w') as f:
            json.dump(sampling_info, f, indent=2)
    
    cell_type_names = {
        1: "Neoplastic", 2: "Inflammatory", 3: "Connective", 4: "Dead", 5: "Epithelial"
    }
    
    cell_type_colors = {
        1: '#FF6B6B', 2: '#4ECDC4', 3: '#45B7D1', 4: '#96CEB4', 5: '#FECA57'
    }
    
    # ä¸ºé€‰ä¸­çš„WSIåˆ›å»ºä¼˜åŒ–çš„è¯¦ç»†åˆ†æ
    for wsi_idx, analysis in enumerate(selected_analyses):
        original_idx = valid_analyses.index(analysis) + 1  # è·å–åŸå§‹ç´¢å¼•
        print(f"Creating optimized detailed analysis for WSI {original_idx} (sample {wsi_idx + 1}/{len(selected_analyses)})...")
        
        # æ”¶é›†æ‰€æœ‰patchçš„ä¿¡æ¯
        all_patches = analysis['patch_analyses']
        selected_patches = [p for p in all_patches if p['rpsm_selected']]
        improved_selected = [p for p in all_patches if p.get('improved_rpsm_selected', False)]
        angio_selected = [p for p in all_patches if p.get('angio_rpsm_selected', False)]
        hybrid_selected = [p for p in all_patches if p.get('hybrid_rpsm_selected', False)]
        
        # è®¡ç®—ä¸´åºŠç›¸å…³æ€§æŒ‡æ ‡
        def calculate_clinical_scores(patches):
            if not patches:
                return {'angiogenesis': 0, 'immune': 0, 'tumor_burden': 0, 'quality': 0}
            
            angio_scores = []
            immune_scores = []
            tumor_scores = []
            quality_scores = []
            
            for patch in patches:
                # è¡€ç®¡ç”Ÿæˆè¯„åˆ†
                if 'cell_ratios' in patch:
                    angio_score = infer_angiogenesis_from_cells(patch['cell_ratios'])
                    angio_scores.append(angio_score)
                
                # å…ç–«æµ¸æ¶¦è¯„åˆ† (åŸºäºç‚ç—‡ç»†èƒæ¯”ä¾‹)
                immune_ratio = patch['cell_ratios'].get('2', 0.0)
                immune_score = min(immune_ratio * 4, 1.0)  # æ ‡å‡†åŒ–åˆ°0-1
                immune_scores.append(immune_score)
                
                # è‚¿ç˜¤è´Ÿè·è¯„åˆ†
                tumor_ratio = patch['cell_ratios'].get('1', 0.0)
                tumor_score = tumor_ratio
                tumor_scores.append(tumor_score)
                
                # è´¨é‡è¯„åˆ† (ç»†èƒå¯†åº¦ + å¤šæ ·æ€§)
                total_cells = patch['total_cells']
                cell_types = sum(1 for count in patch['cell_counts'].values() if count > 0)
                quality_score = (min(total_cells / 200.0, 1.0) * 0.6) + (cell_types / 5.0 * 0.4)
                quality_scores.append(quality_score)
            
            return {
                'angiogenesis': np.mean(angio_scores) if angio_scores else 0,
                'immune': np.mean(immune_scores) if immune_scores else 0,
                'tumor_burden': np.mean(tumor_scores) if tumor_scores else 0,
                'quality': np.mean(quality_scores) if quality_scores else 0
            }
        
        # è®¡ç®—å„æ–¹æ³•çš„ä¸´åºŠè¯„åˆ†
        all_scores = calculate_clinical_scores(all_patches)
        orig_scores = calculate_clinical_scores(selected_patches)
        impr_scores = calculate_clinical_scores(improved_selected)
        angio_scores = calculate_clinical_scores(angio_selected)
        hybrid_scores = calculate_clinical_scores(hybrid_selected)
        
        # åˆ›å»ºä¼˜åŒ–çš„3x2å¸ƒå±€
        fig, axes = plt.subplots(3, 2, figsize=(16, 18))
        
        # 1. è´¨é‡åˆ†å¸ƒçƒ­å›¾ (å·¦ä¸Š) - æ›¿ä»£ç®€å•ç›´æ–¹å›¾
        ax = axes[0, 0]
        
        # å‡†å¤‡äºŒç»´æ•°æ®ï¼šå¯†åº¦ vs å¤šæ ·æ€§
        densities = []
        diversities = []
        selection_status = []
        
        for patch in all_patches:
            density = patch['total_cells']
            diversity = sum(1 for count in patch['cell_counts'].values() if count > 0)
            densities.append(density)
            diversities.append(diversity)
            
            # æ ‡è®°é€‰æ‹©çŠ¶æ€
            if patch['rpsm_selected']:
                selection_status.append('Original')
            elif patch.get('improved_rpsm_selected', False):
                selection_status.append('Improved')
            elif patch.get('angio_rpsm_selected', False):
                selection_status.append('Angiogenesis')
            elif patch.get('hybrid_rpsm_selected', False):
                selection_status.append('Hybrid')
            else:
                selection_status.append('Unselected')
        
        # åˆ›å»ºæ•£ç‚¹å›¾
        colors = {'Original': '#FF6B6B', 'Improved': '#4ECDC4', 'Angiogenesis': '#45B7D1', 
                 'Hybrid': '#96CEB4', 'Unselected': '#DDDDDD'}
        
        for status in ['Unselected', 'Original', 'Improved', 'Angiogenesis', 'Hybrid']:
            mask = [s == status for s in selection_status]
            if any(mask):
                x_data = [densities[i] for i in range(len(mask)) if mask[i]]
                y_data = [diversities[i] for i in range(len(mask)) if mask[i]]
                ax.scatter(x_data, y_data, c=colors[status], label=status, 
                          alpha=0.7 if status != 'Unselected' else 0.3, s=30)
        
        ax.set_xlabel('Cell Density (total cells per patch)')
        ax.set_ylabel('Cell Diversity (number of cell types)')
        ax.set_title(f'WSI {wsi_idx+1}: Patch Quality Landscape')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ ‡æ³¨é«˜è´¨é‡ä½†æœªè¢«é€‰ä¸­çš„patches
        high_quality_unselected = 0
        for i, (d, div, status) in enumerate(zip(densities, diversities, selection_status)):
            if status == 'Unselected' and d >= 100 and div >= 4:
                high_quality_unselected += 1
        
        if high_quality_unselected > 0:
            ax.text(0.02, 0.98, f'âš ï¸ {high_quality_unselected} high-quality patches missed', 
                   transform=ax.transAxes, va='top', 
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
        
        # 2. ä¸´åºŠç›¸å…³æ€§é›·è¾¾å›¾ (å³ä¸Š) - æ–°å¢é‡è¦åŠŸèƒ½
        ax = axes[0, 1]
        
        # 2. ä¸´åºŠç›¸å…³æ€§é›·è¾¾å›¾ (å³ä¸Š) - æ–°å¢é‡è¦åŠŸèƒ½
        ax = axes[0, 1]
        
        # é›·è¾¾å›¾æ•°æ®å‡†å¤‡
        categories = ['Angiogenesis', 'Immune\nInfiltration', 'Tumor\nBurden', 'Patch\nQuality']
        
        # è®¡ç®—åŒç»„å¹³å‡å€¼ä½œä¸ºåŸºå‡†
        same_label_analyses = [a for a in valid_analyses if a['label'] == analysis['label']]
        if len(same_label_analyses) > 1:
            baseline_scores = []
            for other_analysis in same_label_analyses:
                if other_analysis != analysis:
                    other_patches = other_analysis['patch_analyses']
                    other_scores = calculate_clinical_scores(other_patches)
                    baseline_scores.append([other_scores['angiogenesis'], other_scores['immune'], 
                                          other_scores['tumor_burden'], other_scores['quality']])
            baseline = np.mean(baseline_scores, axis=0).tolist() if baseline_scores else [0.3, 0.3, 0.3, 0.3]
        else:
            baseline = [0.3, 0.3, 0.3, 0.3]  # é»˜è®¤åŸºå‡†
        
        current_values = [all_scores['angiogenesis'], all_scores['immune'], 
                         all_scores['tumor_burden'], all_scores['quality']]
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        # ç¡®ä¿æ•°æ®æ•°ç»„é•¿åº¦åŒ¹é…
        current_values_closed = current_values + current_values[:1]
        baseline_closed = baseline + baseline[:1]
        
        ax.plot(angles, current_values_closed, 'o-', linewidth=2, label=f'WSI {wsi_idx+1}', color='#FF6B6B')
        ax.fill(angles, current_values_closed, alpha=0.25, color='#FF6B6B')
        ax.plot(angles, baseline_closed, 'o--', linewidth=1, label='Same Group Avg', color='#888888')
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 1)
        ax.set_title(f'WSI {wsi_idx+1}: Clinical Relevance Profile')
        ax.legend()
        ax.grid(True)
        
        # æ·»åŠ å“åº”æ ‡ç­¾
        response_label = 'Responder' if analysis['label'] == 1 else 'Non-responder'
        ax.text(0.02, 0.98, f'Label: {response_label}', transform=ax.transAxes, va='top',
               bbox=dict(boxstyle="round,pad=0.3", 
                        facecolor='lightgreen' if analysis['label'] == 1 else 'lightcoral', 
                        alpha=0.7))
        
        # 3. RPSMå†³ç­–æµç¨‹å¯è§†åŒ– (å·¦ä¸­)
        ax = axes[1, 0]
        ax.axis('off')
        
        # åˆ›å»ºå†³ç­–æµç¨‹ç»Ÿè®¡
        total_patches = len(all_patches)
        flow_data = {
            'Total Patches': total_patches,
            'Cell Count Filter': 0,
            'Cell Ratio Filter': 0,
            'Quality Filter': 0,
            'Final Selected': {
                'Original': len(selected_patches),
                'Improved': len(improved_selected), 
                'Angiogenesis': len(angio_selected),
                'Hybrid': len(hybrid_selected)
            }
        }
        
        # åˆ†æç­›é€‰åŸå› 
        for patch in all_patches:
            total_cells = patch['total_cells']
            if total_cells < 50 or total_cells > 300:
                flow_data['Cell Count Filter'] += 1
                continue
            
            tumor_ratio = patch['cell_ratios'].get('1', 0.0)
            if tumor_ratio < 0.3:
                flow_data['Cell Ratio Filter'] += 1
                continue
                
            cell_types = sum(1 for count in patch['cell_counts'].values() if count > 0)
            if cell_types < 3:
                flow_data['Quality Filter'] += 1
        
        # ç»˜åˆ¶æµç¨‹å›¾
        flow_text = f"""RPSM Decision Flow Analysis
        
ğŸ”„ Initial Pool: {total_patches} patches
        
ğŸ“Š Filtering Steps:
â”œâ”€ Cell Count Filter: -{flow_data['Cell Count Filter']} patches
â”œâ”€ Cell Ratio Filter: -{flow_data['Cell Ratio Filter']} patches  
â”œâ”€ Quality Filter: -{flow_data['Quality Filter']} patches
        
âœ… Final Selections:
â”œâ”€ Original RPSM: {flow_data['Final Selected']['Original']} ({flow_data['Final Selected']['Original']/total_patches*100:.1f}%)
â”œâ”€ Improved RPSM: {flow_data['Final Selected']['Improved']} ({flow_data['Final Selected']['Improved']/total_patches*100:.1f}%)
â”œâ”€ Angiogenesis: {flow_data['Final Selected']['Angiogenesis']} ({flow_data['Final Selected']['Angiogenesis']/total_patches*100:.1f}%)
â””â”€ Hybrid RPSM: {flow_data['Final Selected']['Hybrid']} ({flow_data['Final Selected']['Hybrid']/total_patches*100:.1f}%)

ğŸ¯ Best Method: {max(flow_data['Final Selected'], key=flow_data['Final Selected'].get)}
ğŸ“ˆ Max Selection Rate: {max(flow_data['Final Selected'].values())/total_patches*100:.1f}%
"""
        
        ax.text(0.05, 0.95, flow_text, transform=ax.transAxes, fontsize=10,
               verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))
        
        # 4. å…³é”®patcheså±•ç¤º (å³ä¸­)
        ax = axes[1, 1]
        
        # é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„patchesè¿›è¡Œå±•ç¤º
        showcase_patches = []
        
        # é«˜è´¨é‡patches (å‰3ä¸ª)
        quality_sorted = sorted(all_patches, key=lambda p: calculate_patch_quality(p), reverse=True)
        showcase_patches.extend([('High Quality', p, '#2E8B57') for p in quality_sorted[:3]])
        
        # è¡€ç®¡ç”Ÿæˆpatches (å‰2ä¸ª)
        angio_sorted = sorted(all_patches, key=lambda p: calculate_angiogenesis_score(p), reverse=True)
        showcase_patches.extend([('High Angiogenesis', p, '#FF6347') for p in angio_sorted[:2]])
        
        # å…ç–«æµ¸æ¶¦patches (å‰2ä¸ª)  
        immune_sorted = sorted(all_patches, key=lambda p: calculate_immune_infiltration(p), reverse=True)
        showcase_patches.extend([('High Immune', p, '#4169E1') for p in immune_sorted[:2]])
        
        # åˆ›å»ºå±•ç¤ºå›¾
        y_positions = []
        colors = []
        labels = []
        
        for i, (category, patch, color) in enumerate(showcase_patches):
            y_pos = len(showcase_patches) - i - 1
            y_positions.append(y_pos)
            colors.append(color)
            
            # åˆ›å»ºæ ‡ç­¾ä¿¡æ¯
            total_cells = patch['total_cells']
            angio_score = calculate_angiogenesis_score(patch) * 100
            quality_score = calculate_patch_quality(patch) * 100
            
            label = f"{category}\nCells: {total_cells}, Angio: {angio_score:.1f}%, Quality: {quality_score:.1f}%"
            labels.append(label)
        
        # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
        bars = ax.barh(y_positions, [1] * len(showcase_patches), color=colors, alpha=0.7)
        ax.set_yticks(y_positions)
        ax.set_yticklabels(labels, fontsize=9)
        ax.set_xlabel('Representative Patches')
        ax.set_title(f'WSI {wsi_idx+1}: Key Patches Showcase')
        ax.set_xlim(0, 1.2)
        
        # æ·»åŠ é€‰æ‹©çŠ¶æ€æŒ‡ç¤º
        for i, (category, patch, color) in enumerate(showcase_patches):
            selected_methods = []
            if patch in selected_patches:
                selected_methods.append('Orig')
            if patch in improved_selected:
                selected_methods.append('Imp')
            if patch in angio_selected:
                selected_methods.append('Angio')
            if patch in hybrid_selected:
                selected_methods.append('Hyb')
                
            if selected_methods:
                ax.text(1.05, y_positions[i], f"âœ“{'/'.join(selected_methods)}", 
                       va='center', fontsize=8, color='green', weight='bold')
            else:
                ax.text(1.05, y_positions[i], "âœ—None", 
                       va='center', fontsize=8, color='red')
        
        # 5. æ–¹æ³•æ€§èƒ½å¯¹æ¯” (å·¦ä¸‹)
        ax = axes[2, 0]
        
        # è®¡ç®—å„æ–¹æ³•çš„æ€§èƒ½æŒ‡æ ‡
        methods_performance = {
            'Original': {
                'count': len(selected_patches),
                'avg_quality': np.mean([calculate_patch_quality(p) for p in selected_patches]) if selected_patches else 0,
                'avg_angio': np.mean([calculate_angiogenesis_score(p) for p in selected_patches]) if selected_patches else 0
            },
            'Improved': {
                'count': len(improved_selected),
                'avg_quality': np.mean([calculate_patch_quality(p) for p in improved_selected]) if improved_selected else 0,
                'avg_angio': np.mean([calculate_angiogenesis_score(p) for p in improved_selected]) if improved_selected else 0
            },
            'Angiogenesis': {
                'count': len(angio_selected),
                'avg_quality': np.mean([calculate_patch_quality(p) for p in angio_selected]) if angio_selected else 0,
                'avg_angio': np.mean([calculate_angiogenesis_score(p) for p in angio_selected]) if angio_selected else 0
            },
            'Hybrid': {
                'count': len(hybrid_selected),
                'avg_quality': np.mean([calculate_patch_quality(p) for p in hybrid_selected]) if hybrid_selected else 0,
                'avg_angio': np.mean([calculate_angiogenesis_score(p) for p in hybrid_selected]) if hybrid_selected else 0
            }
        }
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾
        performance_categories = ['Count\n(normalized)', 'Quality\nScore', 'Angio\nScore']
        performance_angles = np.linspace(0, 2 * np.pi, len(performance_categories), endpoint=False).tolist()
        performance_angles += performance_angles[:1]
        
        max_count = max([perf['count'] for perf in methods_performance.values()]) or 1
        
        colors_methods = ['#FF9999', '#66B2FF', '#99FF99', '#FFB366']
        for i, (method, perf) in enumerate(methods_performance.items()):
            if perf['count'] > 0:  # åªæ˜¾ç¤ºæœ‰patchesçš„æ–¹æ³•
                values = [
                    perf['count'] / max_count,  # æ ‡å‡†åŒ–è®¡æ•°
                    perf['avg_quality'], 
                    perf['avg_angio']
                ]
                values_closed = values + values[:1]  # é—­åˆæ•°æ®
                
                ax.plot(performance_angles, values_closed, 'o-', linewidth=2, 
                       label=f'{method} ({perf["count"]})', color=colors_methods[i])
                ax.fill(performance_angles, values_closed, alpha=0.15, color=colors_methods[i])
        
        ax.set_xticks(performance_angles[:-1])
        ax.set_xticklabels(performance_categories)
        ax.set_ylim(0, 1)
        ax.set_title(f'WSI {wsi_idx+1}: RPSM Methods Performance')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True)
        
        # 6. å¼‚å¸¸æ£€æµ‹ä¸æ´å¯Ÿ (å³ä¸‹)
        ax = axes[2, 1]
        ax.axis('off')
        
        # å¼‚å¸¸æ£€æµ‹åˆ†æ
        anomalies = []
        insights = []
        
        # 1. ç»†èƒè®¡æ•°å¼‚å¸¸
        cell_counts = [p['total_cells'] for p in all_patches]
        q1, q3 = np.percentile(cell_counts, [25, 75])
        iqr = q3 - q1
        outliers = [p for p in all_patches if p['total_cells'] < q1 - 1.5*iqr or p['total_cells'] > q3 + 1.5*iqr]
        if outliers:
            anomalies.append(f"ğŸ“Š {len(outliers)} patches with abnormal cell counts")
        
        # 2. RPSMæ–¹æ³•é€‰æ‹©å·®å¼‚
        method_counts = [len(selected_patches), len(improved_selected), len(angio_selected), len(hybrid_selected)]
        if max(method_counts) > 0:
            method_variance = np.var(method_counts) / np.mean(method_counts) if np.mean(method_counts) > 0 else 0
            if method_variance > 0.5:
                anomalies.append(f"âš ï¸ High variance in RPSM method selections ({method_variance:.2f})")
        
        # 3. è´¨é‡ä¸é€‰æ‹©ä¸åŒ¹é…
        high_quality_unselected = [p for p in all_patches 
                                 if calculate_patch_quality(p) > 0.7 
                                 and p not in selected_patches 
                                 and p not in improved_selected]
        if high_quality_unselected:
            anomalies.append(f"ğŸ” {len(high_quality_unselected)} high-quality patches missed by RPSM")
        
        # 4. ç”Ÿæˆæ´å¯Ÿ
        if all_scores['angiogenesis'] > 0.6:
            insights.append("ğŸ©¸ Strong angiogenesis signature detected")
        
        if all_scores['immune'] > 0.5:
            insights.append("ğŸ›¡ï¸ High immune infiltration observed")
            
        if all_scores['quality'] < 0.4:
            insights.append("âš ï¸ Generally low patch quality")
            
        # å“åº”é¢„æµ‹æ´å¯Ÿ
        if analysis['label'] == 1:  # Responder
            if all_scores['angiogenesis'] < 0.3:
                insights.append("ğŸ¤” Responder with low angiogenesis - investigate further")
        else:  # Non-responder
            if all_scores['angiogenesis'] > 0.7:
                insights.append("ğŸ¤” Non-responder with high angiogenesis - potential misclassification")
        
        # æ–¹æ³•æ•ˆæœæ´å¯Ÿ
        best_method = max(methods_performance, key=lambda m: methods_performance[m]['count'])
        if methods_performance[best_method]['count'] > total_patches * 0.3:
            insights.append(f"âœ… {best_method} RPSM shows best selection rate")
        else:
            insights.append("âŒ All RPSM methods show low selection rates")
        
        # æ˜¾ç¤ºå¼‚å¸¸å’Œæ´å¯Ÿ
        report_text = f"""WSI {wsi_idx+1}: Anomaly Detection & Insights
        
ğŸš¨ ANOMALIES DETECTED:
"""
        if anomalies:
            for anomaly in anomalies:
                report_text += f"   {anomaly}\n"
        else:
            report_text += "   âœ… No significant anomalies detected\n"
            
        report_text += f"""
ğŸ’¡ KEY INSIGHTS:
"""
        if insights:
            for insight in insights:
                report_text += f"   {insight}\n"
        else:
            report_text += "   ğŸ“‹ Standard patterns observed\n"
            
        # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
        report_text += f"""
ğŸ“ˆ SUMMARY STATISTICS:
   Total Patches: {total_patches}
   Best RPSM: {best_method} ({methods_performance[best_method]['count']} patches)
   Avg Quality: {all_scores['quality']:.3f}
   Avg Angiogenesis: {all_scores['angiogenesis']:.3f}
   Clinical Label: {'Responder' if analysis['label'] == 1 else 'Non-responder'}
"""
        
        ax.text(0.05, 0.95, report_text, transform=ax.transAxes, fontsize=9,
               verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        output_path = os.path.join(output_dir, f'wsi_{wsi_idx+1}_detailed_analysis.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return output_path
        
        reason_counts = {}
        all_reasons = []
        
        # ç»Ÿè®¡å„ç§RPSMçš„é€‰æ‹©åŸå› 
        for patch in selected_patches:
            reason = patch.get('rpsm_reason', 'Unknown')
            all_reasons.append(f"Orig: {reason}")
        
        for patch in improved_selected:
            reason = patch.get('improved_rpsm_reason', 'Selected')
            all_reasons.append(f"Impr: {reason}")
        
        for patch in angio_selected:
            reason = patch.get('angio_rpsm_reason', 'Selected')
            all_reasons.append(f"Angio: {reason}")
        
        if all_reasons:
            reason_counts = pd.Series(all_reasons).value_counts()
            colors = plt.cm.Set3(np.linspace(0, 1, len(reason_counts)))
            
            wedges, texts, autotexts = ax.pie(reason_counts.values, 
                                            labels=reason_counts.index,
                                            colors=colors, autopct='%1.1f%%', 
                                            startangle=90)
            ax.set_title(f'WSI {wsi_idx+1}: RPSM Selection Reasons')
            
            for text in texts:
                text.set_fontsize(8)
            for autotext in autotexts:
                autotext.set_fontsize(7)
                autotext.set_color('white')
                autotext.set_weight('bold')
        
        # 4. ç»†èƒç±»å‹ç›¸å…³æ€§çƒ­å›¾ï¼ˆä»…é€‰ä¸­çš„patchï¼‰
        ax = axes[1, 0]
        
        if selected_patches:
            # æ„å»ºç»†èƒæ¯”ä¾‹æ•°æ®çŸ©é˜µ
            ratio_data = []
            for patch in selected_patches:
                ratio_data.append([patch['cell_ratios'].get(str(j), 0.0) for j in range(1, 6)])
            
            if ratio_data:
                ratio_df = pd.DataFrame(data=ratio_data, 
                                      columns=[cell_type_names[j] for j in range(1, 6)])
                corr_matrix = ratio_df.corr()
                
                im = ax.imshow(corr_matrix.values, cmap='coolwarm', aspect='equal', vmin=-1, vmax=1)
                ax.set_xticks(range(5))
                ax.set_yticks(range(5))
                ax.set_xticklabels([cell_type_names[j+1] for j in range(5)], rotation=45)
                ax.set_yticklabels([cell_type_names[j+1] for j in range(5)])
                ax.set_title(f'WSI {wsi_idx+1}: Cell Type Correlations\n(Original RPSM Selected)')
                
                # æ·»åŠ ç›¸å…³æ€§æ•°å€¼
                for i in range(5):
                    for j in range(5):
                        text = ax.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',
                                     ha="center", va="center", color="black", fontsize=8)
                
                plt.colorbar(im, ax=ax, shrink=0.8)
        
        # 5. ä¸‰ç§RPSMæ–¹æ³•çš„é‡å åˆ†æï¼ˆéŸ¦æ©å›¾é£æ ¼ï¼‰
        ax = axes[1, 1]
        
        # è®¡ç®—é‡å æƒ…å†µ
        patch_selections = {}
        for i, patch in enumerate(all_patches):
            orig = patch['rpsm_selected']
            impr = patch.get('improved_rpsm_selected', False)
            angio = patch.get('angio_rpsm_selected', False)
            patch_selections[i] = (orig, impr, angio)
        
        overlap_counts = {
            'Original only': 0, 'Improved only': 0, 'Angiogenesis only': 0,
            'Orig + Impr': 0, 'Orig + Angio': 0, 'Impr + Angio': 0,
            'All three': 0, 'None': 0
        }
        
        for orig, impr, angio in patch_selections.values():
            if orig and impr and angio:
                overlap_counts['All three'] += 1
            elif orig and impr:
                overlap_counts['Orig + Impr'] += 1
            elif orig and angio:
                overlap_counts['Orig + Angio'] += 1
            elif impr and angio:
                overlap_counts['Impr + Angio'] += 1
            elif orig:
                overlap_counts['Original only'] += 1
            elif impr:
                overlap_counts['Improved only'] += 1
            elif angio:
                overlap_counts['Angiogenesis only'] += 1
            else:
                overlap_counts['None'] += 1
        
        # ç§»é™¤è®¡æ•°ä¸º0çš„ç±»åˆ«
        non_zero_overlaps = {k: v for k, v in overlap_counts.items() if v > 0}
        
        if non_zero_overlaps:
            colors = plt.cm.Set2(np.linspace(0, 1, len(non_zero_overlaps)))
            wedges, texts, autotexts = ax.pie(non_zero_overlaps.values(),
                                            labels=non_zero_overlaps.keys(),
                                            colors=colors, autopct='%1.1f%%',
                                            startangle=90)
            ax.set_title(f'WSI {wsi_idx+1}: RPSM Method Overlaps')
            
            for text in texts:
                text.set_fontsize(9)
            for autotext in autotexts:
                autotext.set_fontsize(8)
                autotext.set_color('white')
                autotext.set_weight('bold')
        
        # 6. é€‰æ‹©æ€§ç»Ÿè®¡æ‘˜è¦
        ax = axes[1, 2]
        ax.axis('off')
        
        # åˆ›å»ºç»Ÿè®¡æ‘˜è¦æ–‡æœ¬
        total_patches = len(all_patches)
        orig_count = len(selected_patches)
        impr_count = len(improved_selected)
        angio_count = len(angio_selected)
        
        orig_rate = orig_count / total_patches * 100 if total_patches > 0 else 0
        impr_rate = impr_count / total_patches * 100 if total_patches > 0 else 0
        angio_rate = angio_count / total_patches * 100 if total_patches > 0 else 0
        
        summary_text = f"""WSI {wsi_idx+1} Summary
        
Total patches: {total_patches}

RPSM Selection Results:
â€¢ Original RPSM: {orig_count} ({orig_rate:.1f}%)
â€¢ Improved RPSM: {impr_count} ({impr_rate:.1f}%)
â€¢ Angiogenesis RPSM: {angio_count} ({angio_rate:.1f}%)

Response label: {'Responder' if analysis['label'] == 1 else 'Non-responder'}

Cell count statistics:
â€¢ Mean: {np.mean(all_counts):.1f}
â€¢ Median: {np.median(all_counts):.1f}
â€¢ Range: {np.min(all_counts)}-{np.max(all_counts)}

Best performing RPSM:
{['Original', 'Improved', 'Angiogenesis'][np.argmax([orig_rate, impr_rate, angio_rate])]} 
({max(orig_rate, impr_rate, angio_rate):.1f}% selection rate)
"""
        
        ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=10,
               verticalalignment='top', fontfamily='monospace',
               bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/wsi_{wsi_idx+1}_detailed_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    print(f"Detailed patch analysis saved to {output_dir} directory")

def create_patch_prediction_visualization(wsi_analyses, output_dir="plots"):
    """
    åˆ›å»ºpatché¢„æµ‹å¯è§†åŒ–ï¼Œå‚ç…§app.pyçš„å®ç°æ–¹å¼
    æ¯ä¸ªWSIé€‰æ‹©3ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„patchè¿›è¡Œå¯è§†åŒ–ï¼Œå¹¶åœ¨åŸå›¾ä¸Šæ ‡æ³¨ç»†èƒ
    """
    import cv2
    import torchvision.transforms as transforms
    import torch.nn.functional as F
    os.makedirs(output_dir, exist_ok=True)
    
    valid_analyses = [a for a in wsi_analyses if a is not None]
    if not valid_analyses:
        return
    
    # å‚ç…§app.pyçš„é¢œè‰²æ˜ å°„
    color_dict = {
        0: [0, 0, 0],       # Background - black
        1: [255, 0, 0],     # Neoplastic - red  
        2: [0, 255, 0],     # Inflammatory - green
        3: [0, 0, 255],     # Connective - blue
        4: [255, 255, 0],   # Dead - yellow
        5: [255, 0, 255],   # Epithelial - magenta
    }
    
    type_names = {
        1: "Neoplastic", 2: "Inflammatory", 3: "Connective", 
        4: "Dead", 5: "Epithelial"
    }
    
    # è®¾ç½®æ¨¡å‹ç”¨äºé‡æ–°æ¨ç†
    print("Setting up model for detailed cell visualization...")
    model_result = setup_pannuke_models()
    if model_result is None:
        print("Failed to setup model for visualization")
        return
    
    if len(model_result) == 3:
        pannuke_model, device, is_multi_gpu = model_result
    else:
        print("Unexpected return format from setup_pannuke_models")
        return
    
    for wsi_idx, analysis in enumerate(valid_analyses):
        print(f"Creating patch prediction visualization for WSI {wsi_idx + 1}...")
        
        # é€‰æ‹©æœ‰ä»£è¡¨æ€§çš„patchè¿›è¡Œå¯è§†åŒ–
        all_patches = analysis['patch_analyses']
        
        # é€‰æ‹©3ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„patch
        selected_patches = []
        
        # 1. é€‰æ‹©ä¸€ä¸ªåŸå§‹RPSMé€‰ä¸­çš„patch
        original_selected = [p for p in all_patches if p.get('rpsm_selected', False)]
        if original_selected:
            # é€‰æ‹©ç»†èƒæ•°é‡è¾ƒå¤šçš„patch
            original_selected.sort(key=lambda x: x['total_cells'], reverse=True)
            selected_patches.append(original_selected[0])
        
        # 2. é€‰æ‹©ä¸€ä¸ªæ”¹è¿›/è¡€ç®¡ç”ŸæˆRPSMé€‰ä¸­ä½†åŸå§‹RPSMæœªé€‰ä¸­çš„patch
        alternative_selected = [p for p in all_patches if 
                               (p.get('improved_rpsm_selected', False) or p.get('angio_rpsm_selected', False)) 
                               and not p.get('rpsm_selected', False) 
                               and p not in selected_patches]
        if alternative_selected:
            alternative_selected.sort(key=lambda x: x['total_cells'], reverse=True)
            selected_patches.append(alternative_selected[0])
        
        # 3. é€‰æ‹©ä¸€ä¸ªæœªè¢«ä»»ä½•RPSMé€‰ä¸­çš„patchä½œä¸ºå¯¹æ¯”
        unselected = [p for p in all_patches if not any([
            p.get('rpsm_selected', False),
            p.get('improved_rpsm_selected', False), 
            p.get('angio_rpsm_selected', False)
        ]) and p not in selected_patches and p['total_cells'] > 0]
        
        if unselected:
            # é€‰æ‹©ç»†èƒæ•°é‡é€‚ä¸­çš„patch
            unselected.sort(key=lambda x: x['total_cells'])
            mid_idx = len(unselected) // 2
            selected_patches.append(unselected[mid_idx])
        
        # å¦‚æœpatchæ•°é‡ä¸å¤Ÿ3ä¸ªï¼Œä»å‰©ä½™çš„valid_patchesä¸­é€‰æ‹©
        while len(selected_patches) < 3 and len(selected_patches) < len(all_patches):
            remaining = [p for p in all_patches if p not in selected_patches and p['total_cells'] > 0]
            if remaining:
                # æŒ‰ç»†èƒæ•°é‡æ’åºï¼Œé€‰æ‹©è¾ƒå¥½çš„
                remaining.sort(key=lambda x: x['total_cells'], reverse=True)
                selected_patches.append(remaining[0])
            else:
                break
        
        if not selected_patches:
            print(f"No valid patches with predictions found for WSI {wsi_idx + 1}")
            continue
        
        # é‡æ–°è¿›è¡Œæ¨ç†ä»¥è·å–è¯¦ç»†çš„ç»†èƒæ ‡æ³¨ä¿¡æ¯
        def get_detailed_predictions(image_path, model, device):
            """é‡æ–°æ¨ç†è·å–è¯¦ç»†çš„ç»†èƒé¢„æµ‹ä¿¡æ¯"""
            try:
                # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
                image = cv2.imread(image_path)
                if image is None:
                    return None, None, None
                
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
                # ç¡®ä¿å›¾åƒå°ºå¯¸ä¸º512x512
                if image_rgb.shape[0] != 512 or image_rgb.shape[1] != 512:
                    image_rgb = cv2.resize(image_rgb, (512, 512))
                
                # è½¬æ¢ä¸ºtensor
                from PIL import Image as PILImage
                image_pil = PILImage.fromarray(image_rgb)
                transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
                ])
                image_tensor = transform(image_pil).unsqueeze(0).to(device)
                
                # æ¨ç†
                model.eval()
                with torch.no_grad():
                    predictions = model(image_tensor)
                    predictions["nuclei_binary_map"] = F.softmax(predictions["nuclei_binary_map"], dim=1)
                    predictions["nuclei_type_map"] = F.softmax(predictions["nuclei_type_map"], dim=1)
                    
                    # è·å–å®ä¾‹å›¾å’Œç»†èƒç±»å‹ä¿¡æ¯
                    if hasattr(model, 'module'):  # DataParallelåŒ…è£…çš„æ¨¡å‹
                        instance_map, instance_types = model.module.calculate_instance_map(predictions, magnification=40)
                    else:
                        instance_map, instance_types = model.calculate_instance_map(predictions, magnification=40)
                
                return image_rgb, instance_map, instance_types
                
            except Exception as e:
                print(f"Error in detailed prediction for {image_path}: {e}")
                return None, None, None
        
        # åˆ›å»ºå¯è§†åŒ– - æ¯ä¸ªpatchæ˜¾ç¤ºåŸå›¾ã€æ ‡æ³¨å›¾å’Œç»†èƒåˆ†å¸ƒ
        n_patches = len(selected_patches)
        fig, axes = plt.subplots(3, n_patches, figsize=(6*n_patches, 15))
        
        if n_patches == 1:
            axes = axes.reshape(-1, 1)
        
        for patch_idx, patch in enumerate(selected_patches):
            patch_path = patch['patch_path']
            
            # è·å–è¯¦ç»†é¢„æµ‹ç»“æœ
            original_image, instance_map, instance_types = get_detailed_predictions(patch_path, pannuke_model, device)
            
            if original_image is None:
                print(f"Could not process image: {patch_path}")
                continue
            
            # å­å›¾1: åŸå§‹å›¾åƒ
            ax1 = axes[0, patch_idx]
            ax1.imshow(original_image)
            
            # æ ‡é¢˜åŒ…å«RPSMçŠ¶æ€
            rpsm_status = []
            if patch.get('rpsm_selected', False):
                rpsm_status.append("âœ“ Orig")
            if patch.get('improved_rpsm_selected', False):
                rpsm_status.append("âœ“ Impr")
            if patch.get('angio_rpsm_selected', False):
                rpsm_status.append("âœ“ Angio")
            
            if not rpsm_status:
                rpsm_status = ["âœ— Not selected"]
            
            status_text = " | ".join(rpsm_status)
            ax1.set_title(f'Patch {patch_idx+1} - Original\nCells: {patch["total_cells"]}\n{status_text}', fontsize=10)
            ax1.axis('off')
            
            # å­å›¾2: å¸¦ç»†èƒæ ‡æ³¨çš„å›¾åƒ
            ax2 = axes[1, patch_idx]
            overlay_img = original_image.copy()
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶ç»†èƒæ ‡æ³¨
            if instance_types is not None and len(instance_types) > 0 and len(instance_types[0]) > 0:
                for cell_id, cell_info in instance_types[0].items():
                    if cell_info['type'] == 0:  # è·³è¿‡èƒŒæ™¯
                        continue
                    
                    # è·å–ç»†èƒé¢œè‰²
                    cell_type = cell_info['type']
                    color = color_dict.get(cell_type, [255, 255, 255])
                    
                    # ç»˜åˆ¶è½®å»“
                    try:
                        contour = np.array(cell_info['contour'], dtype=np.int32)
                        cv2.drawContours(overlay_img, [contour], -1, color, 2)
                        
                        # ç»˜åˆ¶è´¨å¿ƒ
                        centroid = tuple(map(int, cell_info['centroid']))
                        cv2.circle(overlay_img, centroid, 3, color, -1)
                    except Exception as e:
                        print(f"Error drawing cell {cell_id}: {e}")
                        continue
                
                # æ·»åŠ å›¾ä¾‹åˆ°å›¾åƒå³ä¸Šè§’
                legend_height = 130
                legend_width = 160
                legend_start_x = max(0, overlay_img.shape[1] - legend_width - 10)
                legend_start_y = 10
                
                # åˆ›å»ºåŠé€æ˜èƒŒæ™¯
                legend_overlay = overlay_img.copy()
                cv2.rectangle(legend_overlay, 
                             (legend_start_x, legend_start_y), 
                             (legend_start_x + legend_width, legend_start_y + legend_height), 
                             (255, 255, 255), -1)
                
                # æ··åˆå›¾ä¾‹èƒŒæ™¯
                alpha = 0.8
                overlay_img[legend_start_y:legend_start_y + legend_height, 
                           legend_start_x:legend_start_x + legend_width] = \
                    cv2.addWeighted(overlay_img[legend_start_y:legend_start_y + legend_height, 
                                              legend_start_x:legend_start_x + legend_width], 
                                   1 - alpha, 
                                   legend_overlay[legend_start_y:legend_start_y + legend_height, 
                                                legend_start_x:legend_start_x + legend_width], 
                                   alpha, 0)
                
                # æ·»åŠ å›¾ä¾‹æ ‡é¢˜
                cv2.putText(overlay_img, "Cell Types:", 
                           (legend_start_x + 5, legend_start_y + 18), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 0), 1)
                
                # æ·»åŠ æ¯ç§ç»†èƒç±»å‹çš„é¢œè‰²å’Œåç§°
                for i, (cell_type, name) in enumerate(type_names.items()):
                    y_pos = legend_start_y + 35 + i * 20
                    color = color_dict[cell_type]
                    
                    # ç»˜åˆ¶é¢œè‰²çŸ©å½¢
                    cv2.rectangle(overlay_img, 
                                 (legend_start_x + 5, y_pos - 8), 
                                 (legend_start_x + 20, y_pos + 3), 
                                 color, -1)
                    
                    # æ·»åŠ æ–‡å­—
                    cv2.putText(overlay_img, name[:3], 
                               (legend_start_x + 25, y_pos), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 0), 1)
            
            ax2.imshow(overlay_img)
            ax2.set_title(f'Patch {patch_idx+1} - Cell Annotations', fontsize=10)
            ax2.axis('off')
            
            # å­å›¾3: ç»†èƒç±»å‹åˆ†å¸ƒé¥¼å›¾
            ax3 = axes[2, patch_idx]
            cell_counts = patch['cell_counts']
            
            # å‡†å¤‡é¥¼å›¾æ•°æ®
            sizes = []
            labels = []
            colors = []
            
            for cell_type in [1, 2, 3, 4, 5]:
                count = int(cell_counts.get(str(cell_type), 0))
                if count > 0:
                    sizes.append(count)
                    labels.append(f"{type_names[cell_type][:3]}\n({count})")
                    colors.append([c/255.0 for c in color_dict[cell_type]])
            
            if sizes:
                wedges, texts, autotexts = ax3.pie(sizes, labels=labels, colors=colors, 
                                                  autopct='%1.1f%%', startangle=90)
                # è®¾ç½®æ–‡å­—å¤§å°
                for text in texts:
                    text.set_fontsize(8)
                for autotext in autotexts:
                    autotext.set_fontsize(7)
                    autotext.set_color('white')
                    autotext.set_weight('bold')
            else:
                ax3.text(0.5, 0.5, 'No cells\ndetected', ha='center', va='center', 
                        transform=ax3.transAxes, fontsize=12)
                ax3.set_xlim(0, 1)
                ax3.set_ylim(0, 1)
                
            ax3.set_title(f'Cell Distribution', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/wsi_{wsi_idx+1}_patch_predictions.png", 
                   dpi=150, bbox_inches='tight')
        plt.close()
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
    
    print("Enhanced patch prediction visualizations with cell annotations saved to plots directory")



def calculate_cohens_d(group1, group2):
    """
    è®¡ç®—Cohen's dæ•ˆåº”å¤§å°
    """
    n1, n2 = len(group1), len(group2)
    mean1, mean2 = np.mean(group1), np.mean(group2)
    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
    
    # åˆå¹¶æ ‡å‡†å·®
    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
    
    if pooled_std == 0:
        return 0
    
    d = (mean1 - mean2) / pooled_std
    return d

def evaluate_rpsm_methods(wsi_analyses):
    """
    å…¨é¢è¯„ä¼°å››ç§RPSMæ–¹æ³•çš„æ€§èƒ½
    """
    responder_analyses = [a for a in wsi_analyses if a is not None and a['label'] == 1]
    non_responder_analyses = [a for a in wsi_analyses if a is not None and a['label'] == 0]
    
    if not responder_analyses or not non_responder_analyses:
        print("è­¦å‘Š: ç¼ºå°‘å“åº”è€…æˆ–éå“åº”è€…æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå®Œæ•´è¯„ä¼°")
        return None
    
    methods = {
        'Original': ('rpsm_selected_count', 'rpsm_selected_patches'),
        'Improved': ('improved_rpsm_selected_count', 'improved_rpsm_selected_patches'),
        'Angiogenesis': ('angio_rpsm_selected_count', 'angio_rpsm_selected_patches'),
        'Hybrid': ('hybrid_rpsm_selected_count', 'hybrid_rpsm_selected_patches')
    }
    
    evaluation_results = {}
    
    for method_name, (count_key, patches_key) in methods.items():
        # è®¡ç®—ç­›é€‰ç‡
        resp_rates = []
        non_resp_rates = []
        resp_angio_scores = []
        non_resp_angio_scores = []
        resp_quality_scores = []
        non_resp_quality_scores = []
        
        for a in responder_analyses:
            if count_key in a and a['total_patches'] > 0:
                rate = a[count_key] / a['total_patches']
                resp_rates.append(rate)
                
                # è®¡ç®—è¡€ç®¡ç”Ÿæˆä¸€è‡´æ€§
                if patches_key in a and a[patches_key]:
                    angio_scores = [infer_angiogenesis_from_cells(p['cell_ratios']) for p in a[patches_key]]
                    resp_angio_scores.extend(angio_scores)
                    
                    # è®¡ç®—è´¨é‡åˆ†æ•°
                    quality_scores = [calculate_patch_quality_score(p['cell_counts']) for p in a[patches_key]]
                    resp_quality_scores.extend(quality_scores)
                else:
                    print(f"Warning: No patches found for {method_name} responder analysis, patches_key: {patches_key}")
        
        for a in non_responder_analyses:
            if count_key in a and a['total_patches'] > 0:
                rate = a[count_key] / a['total_patches']
                non_resp_rates.append(rate)
                
                if patches_key in a and a[patches_key]:
                    angio_scores = [infer_angiogenesis_from_cells(p['cell_ratios']) for p in a[patches_key]]
                    non_resp_angio_scores.extend(angio_scores)
                    
                    quality_scores = [calculate_patch_quality_score(p['cell_counts']) for p in a[patches_key]]
                    non_resp_quality_scores.extend(quality_scores)
                else:
                    print(f"Warning: No patches found for {method_name} non-responder analysis, patches_key: {patches_key}")
        
        if resp_rates and non_resp_rates:
            # ç»Ÿè®¡æµ‹è¯•
            t_stat, p_value = stats.ttest_ind(resp_rates, non_resp_rates)
            effect_size = calculate_cohens_d(resp_rates, non_resp_rates)
            
            # è¡€ç®¡ç”Ÿæˆä¸€è‡´æ€§
            angio_consistency = {}
            if resp_angio_scores and non_resp_angio_scores:
                angio_t_stat, angio_p_value = stats.ttest_ind(resp_angio_scores, non_resp_angio_scores)
                angio_effect_size = calculate_cohens_d(resp_angio_scores, non_resp_angio_scores)
                angio_consistency = {
                    'resp_mean': np.mean(resp_angio_scores),
                    'non_resp_mean': np.mean(non_resp_angio_scores),
                    'difference': np.mean(resp_angio_scores) - np.mean(non_resp_angio_scores),
                    'effect_size': angio_effect_size,
                    'p_value': angio_p_value
                }
            
            # è´¨é‡åˆ†æ•°å¯¹æ¯”
            quality_comparison = {}
            if resp_quality_scores and non_resp_quality_scores:
                quality_t_stat, quality_p_value = stats.ttest_ind(resp_quality_scores, non_resp_quality_scores)
                quality_effect_size = calculate_cohens_d(resp_quality_scores, non_resp_quality_scores)
                quality_comparison = {
                    'resp_mean': np.mean(resp_quality_scores),
                    'non_resp_mean': np.mean(non_resp_quality_scores),
                    'difference': np.mean(resp_quality_scores) - np.mean(non_resp_quality_scores),
                    'effect_size': quality_effect_size,
                    'p_value': quality_p_value
                }
            
            evaluation_results[method_name] = {
                'selection_rate': {
                    'responder_mean': np.mean(resp_rates),
                    'responder_std': np.std(resp_rates),
                    'non_responder_mean': np.mean(non_resp_rates),
                    'non_responder_std': np.std(non_resp_rates),
                    'difference': np.mean(resp_rates) - np.mean(non_resp_rates),
                    'effect_size': effect_size,
                    'p_value': p_value
                },
                'angiogenesis_consistency': angio_consistency,
                'quality_comparison': quality_comparison,
                'discrimination_ratio': np.mean(resp_rates) / (np.mean(non_resp_rates) + 1e-8)
            }
    
    return evaluation_results

def calculate_patch_quality_score(cell_counts):
    """
    è®¡ç®—patchçš„ç»¼åˆè´¨é‡åˆ†æ•°
    """
    total_cells = sum(cell_counts.values())
    if total_cells == 0:
        return 0
    
    # è®¡ç®—ç»†èƒæ¯”ä¾‹
    cell_ratios = {cell_type: count / total_cells for cell_type, count in cell_counts.items()}
    
    # ç»†èƒå¯†åº¦è¯„åˆ† (0-0.3)
    density_score = min(total_cells / 200.0, 1.0) * 0.3
    
    # ç»†èƒå¤šæ ·æ€§è¯„åˆ† (0-0.3)
    cell_types = sum(1 for count in cell_counts.values() if count > 0)
    diversity_score = (cell_types / 5.0) * 0.3
    
    # è¡€ç®¡ç”Ÿæˆç›¸å…³æ€§è¯„åˆ† (0-0.4)
    angio_score = infer_angiogenesis_from_cells(cell_ratios) * 0.4
    
    return density_score + diversity_score + angio_score


def calculate_distribution_overlap(group1, group2):
    """
    è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„é‡å ç¨‹åº¦
    """
    if not group1 or not group2:
        return 0
    
    min_val = min(min(group1), min(group2))
    max_val = max(max(group1), max(group2))
    
    # åˆ›å»ºç›´æ–¹å›¾
    bins = np.linspace(min_val, max_val, 50)
    hist1, _ = np.histogram(group1, bins=bins, density=True)
    hist2, _ = np.histogram(group2, bins=bins, density=True)
    
    # è®¡ç®—é‡å é¢ç§¯
    overlap = np.sum(np.minimum(hist1, hist2)) * (bins[1] - bins[0])
    
    return overlap


def main():
    """
    ä¸»å‡½æ•° - å¢å¼ºç‰ˆæ”¯æŒå¤šGPUåŠ é€Ÿ
    """
    # ç¡®ä¿æ¯æ¬¡è¿è¡Œéƒ½æœ‰ä¸åŒçš„éšæœºç§å­
    import time
    # ä½¿ç”¨æ›´å¤æ‚çš„éšæœºç§å­ç”Ÿæˆæ–¹æ³•
    random_seed = int(time.time() * 1000000) % 2147483647  # ä½¿ç”¨å¾®ç§’çº§æ—¶é—´æˆ³
    random.seed(random_seed)
    np.random.seed(random_seed)
    print(f"Using random seed: {random_seed}")
    
    # é¢å¤–çš„éšæœºçŠ¶æ€é‡ç½®
    torch.manual_seed(random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(random_seed)
        torch.cuda.manual_seed_all(random_seed)
    
    # æ•°æ®è·¯å¾„
    csv_path = "slide_ov_response.csv"
    
    print("WSI Cell Distribution Analysis Script")
    print("="*50)
    
    # Check if CSV file exists
    if not os.path.exists(csv_path):
        print(f"CSV file does not exist: {csv_path}")
        return
    
    # Setup and load models (multi-GPU or single-GPU)
    print("1. Loading PanNuke model...")
    model_result = setup_pannuke_models()
    if model_result is None:
        return
    
    # Unpack the result
    if len(model_result) == 3:
        pannuke_model, device, is_multi_gpu = model_result
    else:
        print("Unexpected return format from setup_pannuke_models")
        return
    
    if is_multi_gpu:
        print(f"ğŸš€ Multi-GPU mode activated with DataParallel")
        print(f"ğŸ’¡ Expected speed improvement with parallel processing")
    else:
        print("ğŸ“± Single-GPU mode with optimized batch size")
    
    # Choose analysis mode: 'all' for complete evaluation, 'sample' for subset analysis, 'load' for loading existing results
    #ANALYSIS_MODE = 'all'    # ğŸ¯ FOR COMPREHENSIVE RPSM OPTIMIZATION: åˆ†ææ‰€æœ‰WSIæ•°æ®
    #ANALYSIS_MODE = "load"  # ä»å·²ä¿å­˜çš„ç»“æœä¸­åŠ è½½æ•°æ®è¿›è¡Œè¯„ä¼°  
    ANALYSIS_MODE = "sample"  # åˆ†æéƒ¨åˆ†WSIæ ·æœ¬ï¼Œæµ‹è¯•RPSMç­›é€‰æ ‡å‡†
    
    print(f"ğŸ“Š Analysis mode: {ANALYSIS_MODE.upper()}")
    
    if ANALYSIS_MODE == 'load':
        # Load existing analysis results
        print("\n2. Loading existing analysis results...")
        import glob
        
        # æŸ¥æ‰¾æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶
        report_files = glob.glob('reports/wsi_analysis_data_*.json')
        if report_files:
            latest_file = max(report_files)
            print(f"ğŸ“‚ Loading data from: {latest_file}")
            
            try:
                with open(latest_file, 'r') as f:
                    data = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„ wsi_analyses æ•°æ®
                if 'wsi_analyses' in data:
                    wsi_analyses = data['wsi_analyses']
                    print(f"âœ… Loaded {len(wsi_analyses)} WSI analyses from saved data")
                else:
                    print("âš ï¸  Old data format detected - missing detailed patch data")
                    print("    Please run analysis with ANALYSIS_MODE = 'sample' or 'all' to get complete evaluation")
                    
                    # åˆ›å»ºæœ€å°åŒ–çš„åˆ†ææ•°æ®ç»“æ„ä»¥æ”¯æŒåŸºæœ¬åŠŸèƒ½
                    wsi_analyses = []
                    for sample in data.get('sample_details', []):
                        analysis = {
                            'wsi_path': sample['sample_name'],
                            'label': 1 if sample['label'] == 'responder' else 0,
                            'total_patches': sample['total_patches'],
                            'rpsm_selected_count': sample['rpsm_results']['original']['selected_count'],
                            'improved_rpsm_selected_count': sample['rpsm_results']['improved']['selected_count'],
                            'angio_rpsm_selected_count': sample['rpsm_results']['angiogenesis']['selected_count'],
                            'hybrid_rpsm_selected_count': sample['rpsm_results'].get('hybrid', {}).get('selected_count', 0)
                        }
                        wsi_analyses.append(analysis)
                        
            except Exception as e:
                print(f"âŒ Error loading data: {e}")
                print("    Falling back to sample mode")
                ANALYSIS_MODE = "sample"
        else:
            print("âŒ No existing analysis data found")
            print("    Falling back to sample mode")
            ANALYSIS_MODE = "sample"
    
    if ANALYSIS_MODE == 'all':
        # Load ALL WSI data for complete RPSM evaluation
        print("\n2. Loading ALL WSI data for complete RPSM evaluation...")
        samples = load_all_wsi_data(csv_path)  # åˆ†ææ‰€æœ‰WSIæ ·æœ¬ï¼Œå®Œæ•´è¯„ä¼°RPSMç­›é€‰æ ‡å‡†
    elif ANALYSIS_MODE == "sample":
        # Load sample WSI data for testing
        print("\n2. Loading sample WSI data for testing...")
        # samples = load_and_sample_wsi_data(csv_path, num_samples_per_group=3)  # åˆ†æéƒ¨åˆ†WSIæ ·æœ¬ï¼Œæµ‹è¯•RPSMç­›é€‰æ ‡å‡†
        samples = load_all_wsi_data(csv_path)
    import ipdb;
    if ANALYSIS_MODE != 'load':
        if not samples:
            print("No available samples found")
            return
        
        # Analyze each WSI sample with checkpoint resume capability
        print("\n3. Starting WSI sample analysis...")
        wsi_analyses = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰checkpointæ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
        checkpoint_file = "wsi_analysis_progress.json"
        start_index = 0
        
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, 'r') as f:
                    progress_data = json.load(f)
                    start_index = progress_data.get('completed_count', 0)
                    wsi_analyses = progress_data.get('analyses', [])
                print(f"ğŸ“‚ Found checkpoint: resuming from WSI {start_index + 1}")
            except:
                print("âš ï¸  Checkpoint file corrupted, starting fresh")
                start_index = 0
                wsi_analyses = []
        
        total_start_time = time.time()
        # import ipdb; ipdb.set_trace()
        # ä»checkpointå¼€å§‹å¤„ç†
        for i in range(start_index, len(samples)):
            sample = samples[i]
            sample_start_time = time.time()
            print(f"\nâ±ï¸ Processing WSI {i+1}/{len(samples)}: {sample['slides_name']}")
            
            try:
                # import ipdb; ipdb.set_trace()
                analysis = analyze_wsi_sample(sample, pannuke_model, device, is_multi_gpu)
                wsi_analyses.append(analysis)
                
                # æ¯å¤„ç†å®Œä¸€ä¸ªWSIå°±æ¸…ç†å†…å­˜å¹¶ä¿å­˜è¿›åº¦
                torch.cuda.empty_cache()
                
                # æ›´æ–°è¿›åº¦æ–‡ä»¶
                progress_data = {
                    'completed_count': i + 1,
                    'analyses': wsi_analyses,
                    'timestamp': datetime.now().isoformat()
                }
                with open(checkpoint_file, 'w') as f:
                    json.dump(progress_data, f, default=numpy_json_serializer, indent=2)
                
                sample_time = time.time() - sample_start_time
                remaining_samples = len(samples) - (i + 1)
                estimated_remaining_time = sample_time * remaining_samples
                
                print(f"âœ… WSI {i+1} completed in {sample_time:.1f}s")
                if remaining_samples > 0:
                    print(f"ğŸ“Š Estimated remaining time: {estimated_remaining_time/60:.1f} minutes")
                    
            except Exception as e:
                import traceback
                error_details = traceback.format_exc()
                print(f"âŒ Error processing WSI {i+1}: {type(e).__name__}: {e}")
                print(f"ğŸ“‹ Full error details:\n{error_details}")
                # æ·»åŠ é”™è¯¯çš„WSIåˆ°ç»“æœä¸­ï¼Œä½†ç»§ç»­å¤„ç†
                wsi_analyses.append(None)
                torch.cuda.empty_cache()  # å‘ç”Ÿé”™è¯¯æ—¶ä¹Ÿæ¸…ç†å†…å­˜
                
                # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜è¿›åº¦
                progress_data = {
                    'completed_count': i + 1,
                    'analyses': wsi_analyses,
                    'timestamp': datetime.now().isoformat(),
                    'last_error': str(e)
                }
                with open(checkpoint_file, 'w') as f:
                    json.dump(progress_data, f, default=numpy_json_serializer, indent=2)
        
        total_time = time.time() - total_start_time
        print(f"\nğŸ¯ Total analysis time: {total_time/60:.1f} minutes")
        
        if is_multi_gpu:
            num_gpus = torch.cuda.device_count()
            single_gpu_estimated = total_time * num_gpus
            speedup = single_gpu_estimated / total_time
            print(f"ğŸš€ Multi-GPU speedup achieved: {speedup:.1f}x")
    else:
        print(f"\nâœ… Loaded existing analysis results with {len(wsi_analyses)} WSI samples")
    
   
if __name__ == "__main__":
    main()
